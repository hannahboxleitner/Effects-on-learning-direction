---
title: "Reproducibility study: Effects of Learning Direction in Retrieval Practice on EFL Vocabulary Learning"
subtitle: "Based on data from Terai et al. (2021)"
authors: "Hannah Boxleitner, Beyhan Aliy"
date: last-modified
format:
  html:
    number-sections: true
    embed-resources: true
    fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
---

# Chapter overview

This chapter is based on data published in:

-   Terai, Masato, Junko Yamashita, and Kelly E. Pasich. 2021. EFFECTS
    OF LEARNING DIRECTION IN RETRIEVAL PRACTICE ON EFL VOCABULARY
    LEARNING. Studies in Second Language Acquisition 43(5): 1116–1137.
    <https://doi.org/10.1017/S0272263121000346>.

This chapter will introduce the application of R in Second Language
Acquisition research, using data from Terai et al. (2021). We will go
through the entire data analysis pipeline, from importing the data
(chapter ???) to running logistic regression models and visualizing
results with it. By the end of this chapter, it should become clear how
to conduct and interpret mixed-effects logistics regression models in R
for experimental data.


# Introducing the study

In this chapter, we attempt to reproduce the results of a linguistics
study by Terai et al. (2021). The study focuses on learning directions
in pair-associated vocabulary learning: L2 to L1, where L2 words are
used as stimuli and responses are given in L1, vs. L1 to L2. Opposed to
studying practices, where both a target and the answer are
simultaneously presented, pair-associated learning includes *retrieval*.
Retrieval is defined as the process of accessing stored information and
plays a crucial role in retaining a learned word in memory **(p.
1116-1117 - Literaturangabe?)**. Previous findings regarding the
efficacy of the two types of learning directions are inconsistent. The
study focuses on the relationship between L2 proficiency and the
effectiveness of the two learning directions in paired-associate
learning in L2 vocabulary acquisition, and follows two research
questions: 1. Which learning direction is more effective for vocabulary
acquisition, L1 to L2 learning or L2 to L1 learning? 2. How does L2
proficiency influence the effects of L2 to L1 and L1 to L2 learning?

Therefore, the authors established the hypotheses that there is no
significant difference between the two learning directions and that the
effect of learning direction depends on the learner’s L2 proficiency
**(p. 1121-1122).**

# Retrieving the authors' data

We will use the authors' original data for our reproduction, which they
have made openly accessible on IRIS, a free database for language
research:

-   [Terai, M., Yamashita, J. & Pasich, K. E. (2021). Effects of
    Learning Direction in Retrieval Practice on EFL Vocabulary Learning.
    Studies in Second Language Acquisition. 43(5).
    1116–1137)](https://www.iris-database.org/search/?s_publicationAPAInlineReference=Terai%20et%20al.%20(2021))

In this particular case, you cannot just access the data, but also the
authors' R code. Our further reproduction will use this R code to
investigate and interpret the statistical analysis and results presented
in the paper.

The dataset (dataset1_ssla_20210313.csv) contains data for each
participant and item. Each row includes:

-   Answer: whether the participant gave the correct answer (1 =
    correct, 0 = incorrect)

-   Test: test type (L1 Production or L2 Production)

-   Condition: learning direction (L2-\>L1 or L1-\>L2)

-   ID: participant identifier

-   ItemID: item (word) identifier

-   Vocab: vocabulary test score (L2 proficiency)

-   *more ( L2 frequency?, length, syllables, etc.?)? -\> why not
    included?*

# Importing the data

Before importing the dataset and starting on our project, we need to
load all the packages that we will need. Some of these packages may have
to installed first **(maybe include textbook reference 13.10)**.

```{r}
#| label: setup
#| warning: false

# This chunk loads necessary libraries.

# Load libraries
library(here) 
library(tidyverse)

```

Next, we can import the data. In contrast to the authors' approach in
their code, we used the here package for importing the data. This
package creates paths relative to the top-level directory and therefore
makes it easier to reference files regardless of how they are organized
inside a project.

```{r}
VocabularyA_L1production <- read.csv(file = here("data", "vocaA_L1pro_20210313.csv"))
VocabularyA_L2production <- read.csv(file = here("data", "vocaA_L2pro_20210313.csv"))
VocabularyB_L1production <- read.csv(file = here("data", "vocaB_L1pro_20210313.csv"))
VocabularyB_L2production <- read.csv(file = here("data", "vocaB_L2pro_20210313.csv"))

#First, we will load the dataset
dat<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

#Then, we need to convert the categorical variables we have to factors using the function as.factor()
dat$Test<-as.factor(dat$Test)
dat$Condition<-as.factor(dat$Condition)

#Next, we will contrast the coding (-0.5 vs. +0.5) for categorical predictors.
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)

contrasts(dat$Test)<-my.simple
contrasts(dat$Condition)<-my.simple

#And standardize vocabulary scores (centered and scaled)
dat$s.vocab<-scale(dat$Vocab)

##All these steps set up the dataset for modeling. Converting variables to factors and coding them with contrast ensures that they are interpreted correctly in the regression. Scaling the vocabulary scores helps with interaction terms.
```

# Descriptive statistics

Before we dive into the descriptive statistics conducted in this study,
we need to explain that one part of it will not be reproduced. While
descriptive statistics about the participants (age, years of learning
etc.) are mentioned in the paper by Terai et al., the data to reproduce
these findings cannot be found. It is neither part of the data set
accessible on IRIS, nor does it come up anywhere in the authors' R code.

# Descriptive statistics of the tests and reliability testing

In this chapter, we will attempt to reproduce the authors' descriptive
statistics regarding the two types of post tests and calculate
Cronbach's α to test reliability.

## Reliability analysis

```{r}
# Vocabulary A (L1 production)

# In this chunk, we conduct a reliability analysis for Vocabulary A scores of the L1 production dataset. For this to work, the psych package has to be installed and loaded:
# install.packages("psych", dependencies = TRUE)

library(psych) # -> reliability analysis/testing

# We apply the alpha() function for reliability testing.

alpha(VocabularyA_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary A (L2 production)

# The same function is used to conduct a reliability analysis for Vocabulary A of the L2 participants.

alpha(VocabularyA_L2production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L1 production)

# With Vocabulary B, we follow the same procedure.

alpha(VocabularyB_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L2 production)

alpha(VocabularyB_L2production[,-1], warnings=FALSE)

```

These reliability analyses put out Cronbach’s α, a measure of internal
consistency of tests. It indicates whether responses are consistent
between items. Before interpreting these results, we will get to the
descriptive statistics of the two types of post-tests, which the
reliability analyses are in regard to.

## Accuracy

```{r}
# To investigate accuracy, we first introduce our dataset as the variable dat.acc and load the dplyr library. In accordance with the previous data imports, we are using the here package in this case as well: 

dat.acc<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

library(dplyr) # -> for data manipulation

```

```{r}
## L1 production test
# L2 -> L1

# This code chunk, referring to L2 -> L1 of the L1 production set, processes the collected results for each participant — their ID, how many items they answered, and how many they got right — and organizes them into a clean table. Therefore, it gives us the core numbers necessary to describe and analyze accuracy. We want to take time to explain this chunk of code in detail. 


#L2→L1 (L1 production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

# The first line of code takes unique IDs from dat.acc and wraps them into a data frame called "ids". The column is then named "id" using the names() function. In the next step, that column is extracted as a vector, meaning that z is a vector of unique IDs. Further, a for loop is used to iterate over elements of this vector z and assigning the IDs to the variable "i". Inside this loop, for each i (a unique ID) the data is filtered for that participant under certain conditions and then the Answer column is selected. The result is stored as the object acc_recT, which is further converted into a vector a. With b and c, the processed answers are computed and lastly, these results are appended into three vectors Score, IDes, and try. 

accu_L2L1_L1Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L1Pro)<-c("ID", "Try", "Score")
accu_L2L1_L1Pro

# In the last part, the three vectors built before are combined into a final data frame, where each vector becomes a column in a table, each row corresponding to one participant. These columns are renamed and the table is printed. If we want to properly see the table outside of the console, we can use the View() function:

View(accu_L2L1_L1Pro)

```

```{r}
#L1→L2 (L1 production test)

ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L1Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L1Pro)<-c("ID", "Try", "Score")
accu_L1L2_L1Pro
```

```{r}
# L1 production

# To provide descriptive statistics, we now want to use the describe() function on the Score column, which returns a rich set of stats.

#L2 → L1 learning
describe(accu_L2L1_L1Pro$Score)

#L1 → L2 learning
describe(accu_L1L2_L1Pro$Score)

# Plot (L1 production)

# To visualize this, we want to create a boxplot comparing the two learning directions. For this, the beeswarm package has to be installed and loaded:

# install.packages("beeswarm")

library(beeswarm)

# For this plot, we want to assign a data frame L1pro that contains the scores from both learning directions in the L1 production test. After changing the names of the columns to how we want them to appear on the x-axis, we create a side-by-side boxplot for the learning conditions. Finally, we add the beeswarm() function specifying add=T, which lets the individual scores appear as jittered dots that don't overlap.

L1pro<-data.frame(accu_L2L1_L1Pro$Score,accu_L1L2_L1Pro$Score)
names(L1pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L1pro,col="grey91", outline = T)
beeswarm(L1pro,add=T)
```

*Interpretation here (whiskers blabla) also or only in the end with
tables?*

```{r}
## L2 production test
# L2 -> L1

# This chunk of code follows the same steps as before with the L1 production set, using L2 scores.

#L2→L1 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L2L1_L2Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L2Pro)<-c("ID", "Try", "Score")
accu_L2L1_L2Pro
```

```{r}
## L2 production test
# L1 -> L2

#L1→L2 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L2Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L2Pro)<-c("ID", "Try", "Score")
accu_L1L2_L2Pro
```

```{r}
# L2 production

# In this chunk of code, we want to provide descriptive statistics for L2 production test as well and visualize them in a similar boxplot.

# L2 -> L1 learning

describe(accu_L2L1_L2Pro$Score)

# L1 -> L2 learning

describe(accu_L1L2_L2Pro$Score)

# As with L1, we create a boxplot comparing the two learning directions in L2 production using the beeswarm package.

#Plot
L2pro<-data.frame(accu_L2L1_L2Pro$Score,accu_L1L2_L2Pro$Score)
names(L2pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L2pro,col="grey91", outline = T)
beeswarm(L2pro,add=T)

```

```{r}
# Summarized results of descriptive statistics

# In this chunk, we want to create tables that display the summarized results of descriptive statistics, similar to the table used in the paper (p. 1126).

# First, we need to load the knitr and kableExtra packages.

library(knitr)
library(kableExtra)

# Next, we want to assign a variable that contains the scores for both data sets (L1 and L2 production) and use the describe() function to get all the important measures.

descriptive_stats_tests <- data.frame(L1pro, L2pro) |> 
  describe()

# In keeping with the table in the paper, the first two columns (number of observations n = 28, and vars) are removed. Also, the rows are renamed in a more readable way.

descriptive_stats_tests_trimmed <- descriptive_stats_tests |> 
  select(-n, -vars, -trimmed, -mad, -range, -se)

rownames(descriptive_stats_tests_trimmed) <- c("L2 → L1 (L1pro)", "L1 → L2 (L1pro)", "L2 → L1 (L2pro)", "L1 → L2 (L2pro)")

# Now we want to display the results in a clean, formatted table using the kable() function.

kable(descriptive_stats_tests_trimmed, 
      caption = "Descriptive Statistics for Test Scores",
      digits = 2) |> 
  pack_rows(index = c("L1 production test" = 2, "L2 production test" = 2)) |> 
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))


# Now we have a table that summarizes the descriptive statistics of the two types of post-tests, where the measures of scores in the two production tests can be easily compared. In the L1 production test, scores were shown to be generally higher than in L2 production test.
```

```{r}
# In this chunk, we want to summarize results of the reliability testing and reproduce a table similar to the one in the paper (p. 1126).

alpha_A_L1 <- alpha(VocabularyA_L1production[,-1], warnings=FALSE)
alpha_A_L2 <- alpha(VocabularyA_L2production[,-1], warnings=FALSE)
alpha_B_L1 <- alpha(VocabularyB_L1production[,-1], warnings=FALSE)
alpha_B_L2 <- alpha(VocabularyB_L2production[,-1], warnings=FALSE)



alpha_table <- data.frame(
  Vocabulary = c("Vocabulary A", "Vocabulary B"),
  
  Alpha_L1 = c(
    sprintf("%.2f", alpha_A_L1$total$raw_alpha),
    sprintf("%.2f", alpha_B_L1$total$raw_alpha)
  ),
  CI_L1 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L1$feldt$lower.ci, alpha_A_L1$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L1$feldt$lower.ci, alpha_B_L1$feldt$upper.ci)
  ),
  
  Alpha_L2 = c(
    sprintf("%.2f", alpha_A_L2$total$raw_alpha),
    sprintf("%.2f", alpha_B_L2$total$raw_alpha)
  ),
  CI_L2 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L2$feldt$lower.ci, alpha_A_L2$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L2$feldt$lower.ci, alpha_B_L2$feldt$upper.ci)
  ),
  
  stringsAsFactors = FALSE
)

# Rename column headers (use empty string for "Vocabulary")

colnames(alpha_table) <- c("", "Alpha", "95% CI", "Alpha", "95% CI")

# Now we want to display the results in a clean, formatted table.

alpha_table %>%
  kable(align = "lcccc", caption = "Alpha coefficients for L1 and L2 production test") %>%
  add_header_above(c(" " = 1, "L1 production" = 2, "L2 production" = 2))

# Rundungsfehler?? why? vllt weil wir bereits in data frame gerundet haben? versuche dataframe mit 3f und dann erst in Tabelle runden

#whether responses are consistent between items
#*We want to associate the results of our analysis here with the interpretation in the study. As we can see in the results of the reliability analysis, which are also displayed in a clean table in the paper (see p.1126), alpha coefficients range from .73 to .84, showing adequate reliability of all the tests.
```

# Effects of Learning Condition (Research Question 1)

```{r}
##Test Type & Learning Direction
# To test RQ1, we fit a logistic mixed-effects regression model predicting accuracy (Answer) from Test, Condition, and their interaction.

#First we install and load the packages we need, these seem to be from an older Rstudio version but they still work just fine!

# install.packages("lme4", discrepancies = TRUE)
# install.packages("effects", discrepancies = TRUE)
# install.packages("emmeans", discrepancies = TRUE)
# install.packages("phia", discrepancies = TRUE)
library(lme4) ##leading required package:Matrix/ this package is for mixed models
library(effects) # -> package is for plotting model effects
library(emmeans) # -> package is for post-hoc comparisons
library(phia) # same as emmeans


#Model with interaction
fit1<-glmer(Answer~Test+Condition+(1|ID)+(1|ItemID),family=binomial, data=dat, glmerControl(optimizer="bobyqa"))

summary(fit1)

#glmer() fits a generalized linear mixed-effects model
#Answer is a binary outcome 
#Test and Condition are fixed effects
#(1|ID) and (1|ItemID) are random intercepts for participants and items
```

# Effects of Learning Directions and L2 Proficiency (Research Question 2)


How does L2 proficiency influence the effects of L2->L1 and L1->L2 learning?

The authors hypothesized that the effect of learning direction might be dependant on the learner's proficiency in English. Meaning, lower-proficiency learners might benefit more from practicing in the L2->L1 direction, whilst higher-proficiency learners might benefit more from the L1->L2 direction.

To answer and test this question, the authors fit generalized linear mixed models (GLMMs) separately for the L1 and L2 production test.

GLMMs are appropriate and quite useful here because:
- The dependent variable (Answer) is binary (correct=1, incorrect=0).
- Responses are nested: the same participants answer multiple items, and the same items are seen by multiple participants.

The packages that we need here are:
- dplyr for the function filter() and general data manipulation,
- lme4 for the function glmer() for fitting GLMMs,
- effects for the function allEffects(), for plotting model predictions,
- phia, for the function testInteractions(), for probing interactions.

The paper reports that the advantage flips by proficiency level
(lower-proficiency learners tend to benefit more from L2-\>L1, higher
proficiency from L1-\>L2).



##L1 Production test

## With a second model the scores of the L1 production test were analyzed. This seccond model contained Learning Condition and Vocabulary Size (English proficiency) as explanatory variables and Accuracy of the L1 production test as the response variable (Terai et al. 2021 : 1127). Here the authors analyzed the data with interaction and without interaction, to find out the AIC, p and z scores there are a few steps we need to take. 

We begin by focusing only on the L1 production data. This can be done by first loading our data with the command read.csv() and then the command filter() which only keeps the rows for the L1 Production test.

```{r}
##L1 Production test

#First, we filter the data so that only the relevant test type (L1 or L2 Production) remains. Then, we ensure that categorical predictors are properly set as factors and that numeric predictors are scaled (standardized).

dat1 <- read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)


##With the filter(...) function, provided by the dplyr package, we only keep the rows for the L1 Production posttests, because we will analyze this test separately from the L2 production test. Meaning, the function is mainly used to subset rows in a data frame based on our specified conditions, (L1 production Test)
dat1.L1<-filter(dat1, Test=="L1 Production")



#Make the categorical factors factorial: this means qualitative data is encoded into numeric data so that we can work with it and use the data for statistical analysis.. "condition" also encodes learning direction. The as.factor() function is used to convert a vector into a factor, which is a data type specifically designed for categorical data. It is essential to use factors for statistical data analysis such as an ANOVA or a regression which we will be doing in a later step. At this point, we need factors for a summary of our data.
#Very basic: we set Condition (Learning direction) as a factor; Condition represents the learning direction (L1 to L2 or L2 to L1)
dat1.L1$Condition <- as.factor(dat1.L1$Condition)
```
Because in a later step we will have to take a look at interactions, we set up contrast coding. Using centered contrasts (-0.5, +0.5) makes the intercept easier to interpret as it becomes the grand mean.

```{r}
  
#In the next step we change our coding so the variables are correct and not still working with the data from previous tests. 
#The function contr.treatment() is used to create contrast matrices for categorical variables in regression models. It is the default contrast coding method in R for unordered factors. This way a baseline/reference level is assigned and then it compares all the other levels to this baseline/reference.
c <- contr.treatment(2)


#The matrix() function creates matrices, which are two-dimensioal data structures where all elements must be of the same type (numeric, character, logical...). ncol=1 is the Number of columns.
my.coding <- matrix(rep(1/2,2,ncol=1))

#In this next step... 
my.simple <- c-my.coding

#The print() function can be used to display the value of an object or expression in the console. It can be customized for different object classes. In our case it prints out what we have assigned to my.simple, this shows...? Our Matrix??
print(my.simple)


#contr.treatment(2) starts with treatment coding(?) for a 2-level factor, and then the next two lines subtract a column of 0.5s, creating a simple (meaning centered) contrast:
##  2
## 0.5
##-0.5

##makes main effects easier to interpret and reduces collinearity in interaction models (so wie ich es verstanden habe????)
#The function contrasts() in R specifies or even retrieves the contrast coding for a variable.
#Contrast coding is essential when working with linear models, as this can determine how how levels of a factor are represented as numeric dummy variables.
#A contrast in R is a combination of variables that allows comparison of different treatments.. (Explain more? Understand??)
#By applying contrast coding, the two groups (L1 to L2 and L2 to L1) are coded as -0.5 and +0.5. This coding makes the model's intercept easier to interpret (it becomes the overall mean).
contrasts(dat1.L1$Condition)<-my.simple

```

In the next step, we need to standardize the vocabulary size scores. That way, one unit of s.vocab corresponds to one standard deviation of proficiency

```{r}

#Scaling the Score: Here Vocab scores (L2 proficiency measures) are converted to z-scores. The scale() function standardizes or normalizes data. It centers or scales the columns of our numeric matric/data frame. (EXPLAIN MORE)
#...
dat1.L1$s.vocab<-scale(dat1.L1$Vocab)

```

##Model without interaction

First we test a model which includes learning direction and proficiency as main effects only.

```{r}
#The glmer() function is provided by the lme4() package that we installed and activated (? anderes Wort ?) earlier. We use this function to fit Generalized Linear Mixed Effects Models (GLMMs). These models are extensions of generalized linear models (GLMs) that include random effects and thus allow the modeling of hierarchical or grouped data.
#This first model assumes that learning direction and L2 proficiency each have independent effects on test accuracy
fit2<-glmer(Answer~s.vocab+Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))

summary(fit2)
```

```{r}
options(digits=7)
AIC(fit2)
#This command prints AIC, a fit index (the lower the better)
```

##Interpretation (?)
#The model predicts the probaility of a correct response (Answer), using logistic regression. The random intercepts (1|ID) and (1|ItemID) account for individual differences between participants and between vocabulary items.
#The output of this first model shows that s.vocab is not significant with p = .57, this means that proficiency by itself did not strongly predict L1 Production accuracy
# For Condition the model also showed that it was not significant p = .18, meanign that learning direction alone also did not explain differences.
#So if we look at each predictor separately, there seems to be no clear effect.

So, if we only consider the two predictors separately, there seems to be no effect.

##Model with interaction
What if proficiency only matters depending on the learning direction? To figure this out, we add an interaction term. 

```{r}
fit2.1<-glmer(Answer~s.vocab*Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))
summary(fit2.1)
##addition of s.vocab:Condition which directly tests RQ2: Does the effect of learning direction depend on L2 proficiency? -> here s.vocab:Condition2 is negative and significant (-0.367, p=.009)
```

```{r}
confint(fit2.1)
```

```{r}
options(digits=7)
AIC(fit2.1)
```
The interaction term s.vocab:Condition is significant -0.37, p = .009, which means that the effect of vocabulary proficiency depends on the learning direction.


##Model comparison
To formally check which model fits better using AIC and a likelihood ratio, we use to command anova(), which is an analysis of variance (a statistical formula which compares variances across the means (or average) of different groups).
```{r}
anova(fit2,fit2.1)
##Model comparison: AIC is lower for the interaction model; the LR test is significant, so the interaction model fits better???
```
Model with interaction: AIC = 1311.6
Model without interaction: AIC = 1316.3
χ² = 6.70, p = .009

The model with interaction is a better fit.

##Visualizing the effect

```{r}
##Visualization of the interaction:
plot(allEffects(fit2.1), multiline=T, ci.style="bands",
    xlab="Vocabulary Size", ylab="Accuracy Rate",
    main="L1 Production", lty=c(1,2),
    rescale.axis=F, ylim = c(0,1), colors="black", asp=1)
```

This plot shows that accuracy increases with proficiency. The slope differs by condition. Learners in one learning direction seem to benefit more strongly from larger vocabularies. 

##Testing simple effects

...Text...Explanation... 

```{r}
#Simple main effect
testInteractions(fit2.1, fixed="s.vocab", across="Condition")
```

##Repeat entire process for L2 Production Test ##without Interaction I
think ##hilfe

##L2 Production Test

We now repeat all the same steps for the L2 production test.

```{r}
##Fehler? Kann die Datei wohl nicht öffnen, komisch?
dat2<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"))

dat2.L2<-filter(dat2, Test=="L2 Production")


#Make the categorical factors factorial

#Make the categorical factors factorial: this means qualitative data is encoded into numeric data so that we can work with it and use the data for statistical analysis.. "condition" also encodes learning direction. The as.factor() function is used to convert a vector into a factor, which is a data type specifically designed for categorical data. It is essential to use factors for statistical data analysis such as an ANOVA or a regression which we will be doing in a later step. At this point, we need factors for a summary of our data.

dat1.L1$Condition<-as.factor(dat1.L1$Condition)

  
#In the next step we change our coding so the variables are correct and not still working with the data from previous tests. 
#The function contr.treatment() is used to create contrast matrices for categorical variables in regression models. It is the default contrast coding method in R for unordered factors. This way a baseline/reference level is assigned and then it compares all the other levels to this baseline/reference.
c<-contr.treatment(2)


#The matrix() function creates matrices, which are two-dimensioal data structures where all elements must be of the same type (numeric, character, logical...). ncol=1 is the Number of columns.
my.coding<-matrix(rep(1/2,2,ncol=1))

#In this next step... 
my.simple<-c-my.coding

#The print() function can be used to display the value of an object or expression in the console. It can be customized for different object classes. In our case it prints out what we have assigned to my.simple, this shows...? Our Matrix??
print(my.simple)


#contr.treatment(2) starts with treatment coding(?) for a 2-level factor, and then the next two lines subtract a column of 0.5s, creating a simple (meaning centered) contrast:
##  2
## 0.5
##-0.5

```

```{r}
##makes main effects easier to interpret and reduces collinearity in interaction models (so wie ich es verstanden habe????)
#The function contrasts() in R specifies or even retrieves the contrast coding for a variable.
#Contrast coding is essential when working with linear models, as this can determine how how levels of a factor are represented as numeric dummy variables.
#A contrast in R is a combination of variables that allows comparison of different treatments.. (Explain more? Understand??)
contrasts(dat1.L1$Condition)<-my.simple

#Scaling the Score: Here Vocab scores (L2 proficiency measures) are converted to z-scores. The scale() function standardizes or normalizes data. It centers or scales the columns of our numeric matric/data frame. (EXPLAIN MORE)
dat1.L1$s.vocab<-scale(dat1.L1$Vocab)


#The glmer() function is provided by the lme4() package that we installed and activated (? anderes Wort ?) earlier. We use this function to fit Generalized Linear Mixed Effects Models (GLMMs). These models are extensions of generalized linear models (GLMs) that include random effects and thus allow the modeling of hierarchical or grouped data.
fit2<-glmer(Answer~s.vocab+Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))
summary(fit2)
```

```{r}
options(digits=7)
AIC(fit2)
#This command prints AIC, a fit index (the lower the better)
``` 
##GLMM with Interaction:
#What if vocabulary size only matters depending on the learning direction?
#Next, researchers added an interaction term:
```{r}
fit2.1<-glmer(Answer~s.vocab*Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))
summary(fit2.1)
##addition of s.vocab:Condition which directly tests RQ2: Does the effect of learning direction depend on L2 proficiency? -> here s.vocab:Condition2 is negative and significant (-0.367, p=.009)
            
```
#Here the results showed that the interaction s.vocab:Condition is significant (-0.367, p = 0.009). This means that the effects of proficiency depend on the learning direction. Vocabulary knowledge seems to help more in one condition than in the other.

          
```{r}
confint(fit2.1)
```

```{r}
options(digits=7)
AIC(fit2.1)
```

#Model Comparison
#To formally check whether the model with the interaction is better than the simpler model, we compare them with a likelihodd ratio test:
```{r}
anova(fit2,fit2.1)
##Model comparison: AIC is lower for the interaction model; the LR test is significant, so the interaction model fits better???
```
#Model with interaction: AIC = 1311.6, fits better than without AIC = 1316.3
#Chi-square test: χ²(1) = 6.70, p = .009 -> interaction model is significantly superior



##Plotting/Visualizing the Effect using the effects package
```{r}
##Visualization of the interaction:
plot(allEffects(fit2.1), multiline=T, ci.style="bands",
    xlab="Vocabulary Size", ylab="Accuracy Rate",
    main="L1 Production", lty=c(1,2),
    rescale.axis=F, ylim = c(0,1), colors="black", asp=1)
```
#Here, this plot shows that the accuracy increases with vocabulary size, but the slope differs by condition and that learners in one learning direction benefit more strongly from larger vocabularies.


```{r}
#Simple main effect
testInteractions(fit2.1, fixed="s.vocab", across="Condition")
```


##L2 Production Test
#The same steps are repeated for L2 Production with dat2.L2
#Here the interaction between s.vocab and Condition was marginally significant (p = .07) which suggests that proficiency also mattered, but that the moderating role of learning direction was weaker compared to the L1 Production test.

```{r}
dat2<-read.csv(file = here("data", "dataset1_ssla_20210313.csv", header=T)


dat2.L2<-filter(dat2, Test=="L2 Production") ##Fehler??

#Make the categorical factors factorial
dat2.L2$Condition<-as.factor(dat2.L2$Condition)

#Change coding
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)
```

```{r}
contrasts(dat2.L2$Condition)<-my.simple 

#Scaling the Score 
dat2.L2$s.vocab<-scale(dat2.L2$Vocab)
  
#without interaction
##Fehler aber ich weiß nicht warum Hilfe omg, danach gehts auch leider nicht mehr weiter...
fit3<-glmer(Answer~s.vocab+Condition+(1|ID)+(1|ItemID),family=binomial, data=dat2.L2, glmerControl(optimizer="bobyqa"))
summary(fit3)
```

```{r}
options(digits=7)
AIC(fit3)
```

```{r}
#with interaction
fit3.1<-glmer(Answer~s.vocab*Condition+(1|ID)+(1|ItemID),family=binomial, data=dat2.L2, glmerControl(optimizer="bobyqa"))
summary(fit3.1)
```

```{r}
confint(fit3.1)
```

```{r}
options(digits=7)
AIC(fit3.1)
```

```{r}
#compare AIC
anova(fit3,fit3.1)
```

```{r}
plot(allEffects(fit3.1),multiline=T,ci.style="bands", xlab="Vocabulary Size", ylab="Accuracy Rate",
       main= "L2 Production", lty=c(1,2), rescale.axis=F, ylim = c(0,1),colors="black",asp=1)
```

##Interpretation & Takeaway
#For L1 Production: Learners' prooficiency strongly interats with learning direction. More proficient learners perform better, but especially so in one condition.
#For L2 Production: A similar trend can be seen, although it is less pronounced.
# Vocabulary proficiency shapes how effectively retrieval practice works, and the optimal learning direction may differ for learners with higher vs. lower proficiency.

What you learned in this section:
- Adding a continous predictor (s.vocab) to a GLMM.
- Testing and interpreting interactions.
- Comparing models using AIC and likelihood ratio tests.
- Visualizing interaction effects with the effects package.




# Conclusion

-   No overall difference in accuracy between L1 -\> L2 and L2 -\> L1
    learning (RQ1)
-   A significant interaction between learning direction and L2
    proficiency for L1 production scores (RQ2)
    -   lower-proficiency learners benefited more from L2-\>L1 learning
    -   higher-proficiency learners benefited more from L1-\>L2 learning

Statistical takeaway (maybe???): Mixed-effects logistic regression
allows us to model data with both fixed effects (e.g. condition) and
random effects (e.g. participant and item variability)
