---
title: "Reproducibility study: Effects of Learning Direction in Retrieval Practice on EFL Vocabulary Learning"
subtitle: "Based on data from Terai et al. (2021)"
authors: "Hannah Boxleitner, Beyhan Aliy"
date: last-modified
format:
  html:
    number-sections: true
    embed-resources: true
    fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
---

# Chapter overview

This chapter is based on data published in:

-   Terai, Masato, Junko Yamashita, and Kelly E. Pasich. 2021. EFFECTS
    OF LEARNING DIRECTION IN RETRIEVAL PRACTICE ON EFL VOCABULARY
    LEARNING. Studies in Second Language Acquisition 43(5): 1116–1137.
    <https://doi.org/10.1017/S0272263121000346>.

Based on the data from Terai et al. (2021), we will introduce the
application of R in Second Language Acquisition research. This chapter
will walk you through how to:

-   Retrieve the authors' data and import it in R

-   Conduct descriptive statistics concerning target items, test types
    and alpha coefficients for tests

-   Reproduce tables 3, 4 and 5 in Terai et al. (2021)

-   Fit a generalized linear mixed-effects model to investigate the
    effectiveness of learning direction

-   Compare models with and without interaction

-   Fit a generalized linear mixed-effects model to explore the
    influence of L2 proficiency on the two types of learning

-   Interpret and compare the results with those printed in Terai et al.

    (2021) 

-   Visualize the results with different kinds of plots to facilitate
    interpretation

By the end of this chapter, it should become clear how to fit and
interpret mixed-effects logistics regression models in R for
experimental data.

# Introducing the study

In this chapter, we attempt to reproduce the results of a linguistics
study by Terai et al. (2021). The study focuses on learning directions
in pair-associated vocabulary learning: L2 to L1, where L2 words are
used as stimuli and responses are given in L1, vs. L1 to L2, where it is
the other way around. Opposed to studying practices, where both a target
and the answer are simultaneously presented, pair-associated learning
includes *retrieval*. Retrieval is defined as the process of accessing
stored information and plays a crucial role in retaining a learned word
in memory **(p. 1116-1117 - Literaturangabe?)**. Previous findings
regarding the efficacy of the two types of learning directions are
inconsistent. The study focuses on the relationship between L2
proficiency and the effectiveness of the two learning directions in
paired-associate learning in L2 vocabulary acquisition, and follows two
research questions: 1. Which learning direction is more effective for
vocabulary acquisition, L1 to L2 learning or L2 to L1 learning? 2. How
does L2 proficiency influence the effects of L2 to L1 and L1 to L2
learning?

To answer these questions, the authors designed an experiment where
Japanese EFL learners studied word pairs and then completed retrieval
practive tasks in different conditions. After learning, students were
tested on their ability to produce the target vocabulary items both in
L1 and L2.

((Very basically explained, what the authors tested in the Production
tests consisted of showing the participants either an English or a
Japanese word and then they had to write it in the other language. This
way their abilities to access either their native (L1) or second
language (L2). Think about yourself, when you see a foreign word you
have learned before, how easily can you think of the meaning in your
native language? This is exactly what we are measuring.))

Therefore, the authors established the hypotheses that there is no
significant difference between the two learning directions and that the
effect of learning direction depends on the learner’s L2 proficiency
**(p. 1121-1122).**

# Retrieving the authors' data

We will use the authors' original data for our reproduction, which they
have made openly accessible on IRIS, a free database for language
research:

-   [Terai, M., Yamashita, J. & Pasich, K. E. (2021). Effects of
    Learning Direction in Retrieval Practice on EFL Vocabulary Learning.
    Studies in Second Language Acquisition. 43(5).
    1116–1137)](https://www.iris-database.org/search/?s_publicationAPAInlineReference=Terai%20et%20al.%20(2021))

In this particular case, you cannot just access the data, but also the
authors' R code. Our further reproduction will use this R code to
investigate and interpret the statistical analysis and results presented
in the paper.

The data set (dataset1_ssla_20210313.csv) contains data for each
participant and item, including Answer (whether the participant gave the
correct answer (1 = correct, 0 = incorrect)), Test (distinguishing two
test types: L1 Production or L2 Production), Condition (learning
direction L2 -\> L1 or L1 -\> L2), and so on. But more on that later.

# Importing and setting up the data

Before importing the data set and starting on our project, we need to
load all the packages that we will need. Some of these packages may have
to installed first **(maybe include textbook reference 13.10)**.

```{r}
#| label: setup
#| warning: false

# This chunk loads necessary libraries.

# Load libraries
library(here) 
library(tidyverse)

```

Next, we can import the data. In contrast to the authors' approach in
their code, we used the here package for importing the data. This
package creates paths relative to the top-level directory and therefore
makes it easier to reference files regardless of how they are organized
inside a project.

```{r}
VocabularyA_L1production <- read.csv(file = here("data", "vocaA_L1pro_20210313.csv"))
VocabularyA_L2production <- read.csv(file = here("data", "vocaA_L2pro_20210313.csv"))
VocabularyB_L1production <- read.csv(file = here("data", "vocaB_L1pro_20210313.csv"))
VocabularyB_L2production <- read.csv(file = here("data", "vocaB_L2pro_20210313.csv"))

#First, we will load the data set
dat<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

#Then, we need to convert the categorical variables we have to factors using the function as.factor()
dat$Test<-as.factor(dat$Test)
dat$Condition<-as.factor(dat$Condition)

```

We also apply contrast coding, also called "treatment coding" (Chambers
& Hastie 1992). While it may look confusing and maybe intimidating, it
is actually quite the important step. Basically, we want to compare our
two groups (L1 -\> L2 vs. L2 -\> L1). We use 'simple coding' where our
first Condition (L1 -\> L2) gets coded as -0.5 and our second Condition
2 (L2 -\> L1) as +0.5. This makes interpretations easier when we have
interactions since the intercept represents the overall mean across
conditions (Barlaz 2022).

```{r}
#Next, we will contrast the coding (-0.5 vs. +0.5) for categorical predictors
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)

contrasts(dat$Test)<-my.simple
contrasts(dat$Condition)<-my.simple

#And standardize vocabulary scores (centered and scaled)
dat$s.vocab<-scale(dat$Vocab)

# All these steps set up the data set for modeling. Converting variables to factors and coding them with contrast ensures that they are interpreted correctly in the regression. Scaling the vocabulary scores helps with interaction terms.
```

# Descriptive statistics

Before we dive into the descriptive statistics conducted in this study,
we need to explain that one part of it will not be reproduced. While
descriptive statistics about the participants (age, years of learning
etc.) are mentioned in the paper by Terai et al. (2021), the data to
reproduce these findings cannot be found. It is neither part of the data
set accessible on IRIS, nor does it come up anywhere in the authors' R
code. Therefore, we will only focus on the descriptive statistics of the
target items (40 English words used in the study), test types and the
reliability testing. We'll start with target items, which is found in
Table 3 (p. 1123) in Terai et al. (2021).

Most of the statistics that we will be looking at are things we are
already familiar with: the mean, median, maximum and minimum, as well as
the standard deviation (SD). In addition, the authors also reported
skewness and kurtosis.

Skewness and Kurtosis are both measures of shapes and distribution of a
data set in qualitative methods. Skewness measures the asymmetry of
distribution. Simply put, it describes the amount of which the
distribution differs from a completely symmetric shape.

Skew = 0 -\> perfectly symmetrical. Negative Skew -\> longer "tail" on
the left. Positive Skew -\> longer "tail" on the right.

Kurtosis then is a measure of peaks of a distribution. Basically, it is
a description of how the peaks compare to a normal distribution.

Kurtosis = 0 -\> similar to a normal distribution. Positive kurtosis -\>
sharper peak (more concentrated in the center). Negative kurtosis -\>
flatter peak.

## Table 3 Reproduction

We want to reproduce Table 3 in Terai et al. (2021), which shows us the
descriptive statistics of the target items of the study. Unfortunately,
the code for all the tables was not included in the R code published by
the authors. But since we have the data in the form of a csv file, this
is not a problem! We have the organized data ready for us to analyze.
Thus, we will learn how to reproduce the data of Table 3 step by step.

We have already loaded the data we will need for our table, but just to
be sure, we will do it again and also load all the necessary libraries
for the two tables, so we have a kind of "toolbox" that contains all the
functions we might need:

```{r}
#Install these once if you do not already have them:
#install.packages("readr")
#install.packages("dplyr")
#install.packages("moments")
#install.packages("knitr")
#install.packages("kableExtra")


#Load the packages you need
library(readr) #read csv
library(dplyr) #data wrangling
library(moments) #for skewness and kurtosis
library(knitr) #for the command kable()
library(kableExtra) #for nicer tables

#We load our data which was provided in the IRIS repository
dat <- read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header = TRUE)

#twimws() removes space from the beginning and end of the text which helps to prevent issues like "berth " vs. "berth" being treated as different
dat$ItemID <- trimws(dat$ItemID)
```

In the paper by Terai et al. (2021) we can see that the variables used
are, for L2-related variables: Frequency, Syllables, and Letters. For
L1-related variables: Frequency, Letters, and Mora (syllables) as well
as Familiarity (Fami B). The Fami A and Frequency for L1-related
variables that we see in the table are the familiarity ratings and
frequencies taken from another study by Amano & Kondo (1999).

First, we need to take a look at the data the authors have uploaded for
us. The column names may differ from what we want to see in the table
later on which is why we briefly want to check them by having them
printed:

```{r}
names(dat)
##This is what the columns in our dataset mean:
#ItemID: the English target word (40 items)
#L2Frequency is simply the frequency of the word in English (COCA: Corpus of Contemporary American English)
#Syllables: number of syllables in the English word
#Alphabet: number of letters in the English word (this is "Letters" in the L2 table)
#L1Frequency: frequency of the word in Japanese (Amano & Kondo 1999)
#WordLength: number of letters in the Japanese orthographic form
#mora: number of mora (Japanese syllable-like units)
#Familiarity: familiarity ratings from the current study (Fami (B))

```

Basically, what we are doing next is shaping our data from
participant-level to item-level. The raw data set has multiple rows per
word because each participant contributed responses and thus has their
own row per word. For example, the word "weasel" might appear many
times, one time for each participant. But for descriptive statistics of
the items, we only want one row per word.

To solve this issue we use the commands group_by() and summarise() to
turn all those rows into just a single one per word. The command
'group_by(ItemID)' we use in the code groups all rows together that
contain the same word. The function summarise() then creates our new
dataset by calculating the information for each group, so L2Frequency,
Syllables, Alphabet, etc. The function 'first()' is fairly simple, since
the frequencies, syllables etc. of the words do not change, we just take
the first occurence without any repetition. Only for the familiarity
ratings we calculated the mean() across all participants for each word.

```{r}
items <- dat |>
  group_by(ItemID) |>
  summarise(
    L2Frequency = first(L2Frequency),
    Syllables = first(Syllables),
    Alphabet = first(Alphabet),
    L1Frequency = first(L1Frequency),
    WordLength = first(WordLength),
    mora = first(mora),
    familiarity = mean(familiarity, na.rm = TRUE),
    .groups = "drop"
  )

```

A quick check before we continue, we expect 40 unique items so this
command should print 40:

```{r}
nrow(items)
```

In order to avoid repeating code (and because we are lazy), we create
our own function that computes all the statistics (mean, SD, median,
min, max, skew, kurtosis) we need for each variable. We build our own
small machine that we will name desc(), it will do all the calculations
for us. 'desc \<- funcion(x) creates this function with the input 'x'
which represents whichever variable we want to analyze. Inside of our
function, we use 'data.frame()' for organization of our statistics
before we begin the calculations. Each calculation takes our input
variable 'x' and computes the statistics. The functions skewness() and
kurtosis() come from the 'moments' package we loaded earlier.

```{r}
desc <- function(x) {
  data.frame(
    Mean = mean(x, na.rm = TRUE),
    SD = sd(x, na.rm = TRUE),
    Median = median(x, na.rm = TRUE), 
    Minimum = min(x, na.rm = TRUE),
    Maximum = max(x, na.rm = TRUE),
    Skew = skewness(x, na.rm = TRUE),
    Kurtosis = kurtosis(x, na.rm = TRUE)
    )
}
```

Now that we have most of what we need for the tables, we can start to
build them up! There are two subtables we need.

The code below does this with two important functions: cbind() which
creates columns that stick together side by side. For example
'cbind(Variable = "Frequency", desc(items\$L2Frequency)' creates one row
by combining the columb called "Variable" with the value "Frequency" and
all the statistics from our desc() function applied to the L2 data. The
command rbind() stacks these rows on top of each other to build the
final table. As you see we only have to run our helper function 'desc()'
seven times!

```{r}
tab_L2 <- rbind(
  cbind(Variable = "Frequency", desc(items$L2Frequency)),
  cbind(Variable = "Syllables", desc(items$Syllables)),
  cbind(Variable = "Letters", desc(items$Alphabet))
)

tab_L1 <- rbind(
  cbind(Variable = "Frequency", desc(items$L1Frequency)),
  cbind(Variable = "Letters", desc(items$WordLength)),
  cbind(Variable = "Mora (syllables)", desc(items$mora)),
  cbind(Variable = "Fami (B)", desc(items$familiarity))
)

```

Great! You are almost done building the table for the descriptive
statistics. In the next step, we will round the numbers to two decimals
to make the table more comprehensible.

```{r}
tab_L2$Mean     <- round(tab_L2$Mean, 2)
tab_L2$SD       <- round(tab_L2$SD, 2)
tab_L2$Median   <- round(tab_L2$Median, 2)
tab_L2$Minimum  <- round(tab_L2$Minimum, 2)
tab_L2$Maximum  <- round(tab_L2$Maximum, 2)
tab_L2$Skew     <- round(tab_L2$Skew, 2)
tab_L2$Kurtosis <- round(tab_L2$Kurtosis, 2)

tab_L1$Mean     <- round(tab_L1$Mean, 2)
tab_L1$SD       <- round(tab_L1$SD, 2)
tab_L1$Median   <- round(tab_L1$Median, 2)
tab_L1$Minimum  <- round(tab_L1$Minimum, 2)
tab_L1$Maximum  <- round(tab_L1$Maximum, 2)
tab_L1$Skew     <- round(tab_L1$Skew, 2)
tab_L1$Kurtosis <- round(tab_L1$Kurtosis, 2)
```

Perfect, the next and easiest part of building up our two subtables is
captioning them and then finally printing them. Here we need the kable()
and the kable_classic() function. We use these functions to make the
tables look easy to read. The kable() function is quite nice because it
helps to make R's output look a little more professional. The function
comes from the knitr package. kable(tab_L2, ...) takes our tab_2 data
frame and converts it into a formatted table which we then caption with
the command 'caption = "...". With the command 'align = "lrrrrrr" we
control the way the text is aligned in each column. the 'l' means
left-aligned, while every added 'r' means the remaining columns should
be right-aligned (SOURCE). The kable_classics function comes from the
kableExtra package we loaded earlier. It is also another function to
tidy up our table style. The command 'full_width = FALSE' makes sure
that the table does not stretch too wide. When you run this code, you
will see a polished table with clear borders and proper formatting which
will make it easier to read and interpret.

```{r}
kable(tab_L2, caption = "Table 3a. Descriptive statistics for L2 (English) item properties.",
      align = "lrrrrrr") |>
  kable_classic(full_width = FALSE)

kable(tab_L1, caption = "Table 3b. Descriptive statistics for L1 (Japanese) item properties.",
      align = "lrrrrrr") |>
  kable_classic(full_width = FALSE)
```

## Extension (Beyhan)

### Visualizations of Table 3

With Table 3 we were shown a summary of many numbers (mean, SD, range,
skewness and kurtosis) for the 40 target words, but these numbers are
much easier to understand when we actually see them visualized. Below we
want to turn some key variables of Table 3 into quick histograms to
visually connect the ideas of skew and kurtosis to the actual items used
in this study.

Please note that in this extension our reloading of the data may seem
repetitive, but it simply ensures that this section can work on its own
if needed.

```{r}
library(tidyverse)
dat<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header = TRUE)
dat$ItemID <- trimws(dat$ItemID)

items <- dat |>
  group_by(ItemID) |>
  summarise(
    L2Frequency  = first(L2Frequency),
    Syllables    = first(Syllables),
    Letters_L2   = first(Alphabet),
    L1Frequency  = first(L1Frequency),
    Letters_L1   = first(WordLength),
    Mora         = first(mora),
    Familiarity  = mean(familiarity, na.rm = TRUE),
    .groups = "drop"
  )

##if you want you can quickly check the item number with 'nrow(items)' it should be 40!
```

Now, before creating our histograms, we will add a tiny helper for
style. It will also keep our histograms consistent. The commands
'fill_blue' and 'line_blue' store the colors we want that we will use
for all following histograms. 'base_theme' here creates a cleaner
appearance by removing any unnecessary gridlines and setting a readable
font size.

```{r}
fill_blue <- "#A6D8FF"
line_blue <- "#2B6FA6"
base_theme <- theme_minimal(base_size = 12) + theme(panel.grid.minor = element_blank())
```

We will start with a histogram of L2 Frequencies. To do this we use the
ggplot() command. Our first line of code here 'ggplot(data = items,
aes(x = L2Frequency))' tells ggplot to basically use the 'items' dataset
and put the L2 frequencies of each word onto the x-axis.
'geom_histogram(bins = 15, ...) creates the actual histogram bars,
dividing the data into 15 groups. To fully understand where our numbers
in the table come from, we add reference lines: 'geom_vline(xintercept =
mean(...))' for the mean value, and 'geom_vline(xintercept =
median(...), linetype = "dashed"...)' for a dashed line at the median.

```{r}
ggplot(items, aes(x = L2Frequency)) +
  geom_histogram(bins = 15, fill = fill_blue, color = line_blue, linewidth = 0.4) +
  geom_vline(xintercept = mean(items$L2Frequency), linewidth = 0.7) +
  geom_vline(xintercept = median(items$L2Frequency), linetype = "dashed", linewidth = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of L2 frequencies",
       x = "L2 frequency (COCA)",
       y = "Number of words",
       caption = "Solid line = mean; dashed line = median") +
base_theme
```

In this histogram we can clearly see a right-skewed distribution which
we saw in the table with the positive skewness values (\~1.51). This
means most words are lower or medium frequency with only a few really
high-frequency words (see the long right tail). The mean sits to the
right of the median which is another characteristic of positive skew.

Now, we want to do the exact same for the L1 frequencies to compare our
numbers and outcomes.

```{r}
ggplot(items, aes(x = L1Frequency)) +
  geom_histogram(bins = 15, fill = fill_blue, color = line_blue, linewidth = 0.4) +
  geom_vline(xintercept = mean(items$L1Frequency), linewidth = 0.7) +
  geom_vline(xintercept = median(items$L1Frequency), linetype = "dashed", linewidth = 0.7) +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of L1 frequencies",
       x = "L1 frequency (Amano & Kondo 1999)",
       y = "Number of words",
       caption = "Solid line = mean; dashed line = median") +
base_theme
```

Again, we see a right-skew even stronger than for L2. This means there
are a few very common L1 forms.

Just to be sure we understand how to build and understand these
histograms, we want to create two more. For L2 letters and L1 letters
Japanese orthographic length).

```{r}
ggplot(items, mapping = aes(x = Letters_L2)) +
  geom_histogram(binwidth = 1, boundary = 0.5,
                 fill = fill_blue, color = line_blue, linewidth = 0.4) +
  geom_vline(xintercept = mean(items$Letters_L2), linewidth = 0.7) +
  geom_vline(xintercept = median(items$Letters_L2), linetype = "dashed", linewidth = 0.7) +
  scale_x_continuous(breaks = 3:12) +
  labs(title = "Distribution of letter counts (L2)",
    x = "Letters (L2)", y = "Number of words") +
  base_theme
```

Great! Here we can clearly see that many items are around 5 to 7 letters
with very few short and long words. The table already showed that there
was a slight right-skew which we can now see in the histogram as well
(\~0.97).

Lastly we want to compare the L2 letters to the L1 letters of Japanese.
Here we use nearly the same code:

```{r}
ggplot(items, mapping = aes(x = Letters_L1)) +
  geom_histogram(binwidth = 1, boundary = 0.5,
                 fill = fill_blue, color = line_blue, linewidth = 0.4) +
  geom_vline(xintercept = mean(items$Letters_L1), linewidth = 0.7) +
  geom_vline(xintercept = median(items$Letters_L1), linetype = "dashed", linewidth = 0.7) +
  scale_x_continuous(breaks = 3:12) +
  labs(title = "Distribution of letter counts (L1)",
    x = "Letters (L1)", y = "Number of words") +
  base_theme
```

Amazing, here we clearly see that there is a cluster around 3 to four,
showing that the distribution is quite compact with only a slight right
skew (\~0.17).

Our reproduced tables now closely match Table 3 in Terai et al. (2021)!
The L2 frequency has a mean of about \~1025 with a quite large SD (
\~852), which shows us that some English words were quite frequent while
others were much rarer. The skewness here is positive (\~1.51), although
different than the one reported in the paper (\~1.57). The difference is
only small and still shows that the distribution has a "long tail" of
very high-frequency words. Syllables in English have an average mean of
2, and range from 1 to 5 with only a slight positive skewness which
means that most words are short and only a few are longer, again there
is a small difference in skewness. Letters average around 6.2 again with
a positive skewness. For the L1 variables, the frequency distribution
also shows a strong positive skew, which means that few Japanese
translations are very common, but many are quite rare. Fami (B) averages
about 5.2 on a 7-point scale, with a pretty low SD, suggesting that the
words were mostly familiar.

Our skewness values match the original paper closely, with only small
differences. However, our kurtosis values differ quite a lot from the
published results. This may seem like a huge issue at first, but is
actually quite normal. Kurtosis calculations can vary between packages,
for example the 'moments' package we used calculates excess kurtosis,
while the authors of the original paper might have used different
methods. (SOURCEEE)

In summary, our reproduction shows the same patterns as the published
table: most words are short, quite familiar, and positively skewed in
their distributions. The small differences in skewness and especially in
kurtosis are due to different calculation methods, not because our
analysis is wrong. This is also a nice reminder that in statistics,
sometimes numbers can vary slightly depending on the software or
definitions used, but the overall interpretations remain the same.

## Descriptive statistics of the tests and reliability testing

In this chapter, we will attempt to reproduce the authors' descriptive
statistics regarding the two types of post tests and calculate
Cronbach's α to test reliability.

### Reliability analysis

```{r}
# Vocabulary A (L1 production)

# In this chunk, we conduct a reliability analysis for Vocabulary A scores of the L1 production dataset. For this to work, the psych package has to be installed and loaded:
# install.packages("psych", dependencies = TRUE)

library(psych) # -> reliability analysis/testing

# We apply the alpha() function for reliability testing.

alpha(VocabularyA_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary A (L2 production)

# The same function is used to conduct a reliability analysis for Vocabulary A of the L2 participants.

alpha(VocabularyA_L2production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L1 production)

# With Vocabulary B, we follow the same procedure.

alpha(VocabularyB_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L2 production)

alpha(VocabularyB_L2production[,-1], warnings=FALSE)

```

These reliability analyses put out Cronbach’s α, a measure of internal
consistency of tests. It indicates whether responses are consistent
between items. Before interpreting these results, we will get to the
descriptive statistics of the two types of post-tests, which the
reliability analyses are in regard to.

### Accuracy

```{r}
# To investigate accuracy, we first introduce our data set as the variable dat.acc and load the dplyr library. In accordance with the previous data imports, we are using the here package in this case as well: 

dat.acc<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

library(dplyr) # -> for data manipulation

```

```{r}
## L1 production test
# L2 -> L1

# This code chunk, referring to L2 -> L1 of the L1 production set, processes the collected results for each participant — their ID, how many items they answered, and how many they got right — and organizes them into a clean table. Therefore, it gives us the core numbers necessary to describe and analyze accuracy. We want to take the time to explain this chunk of code in detail. 


#L2→L1 (L1 production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

# The first line of code takes unique IDs from dat.acc and wraps them into a data frame called "ids". The column is then named "id" using the names() function. In the next step, that column is extracted as a vector, meaning that z is a vector of unique IDs. Further, a for loop is used to iterate over elements of this vector z and assigning the IDs to the variable "i". Inside this loop, for each i (a unique ID) the data is filtered for that participant under certain conditions and then the Answer column is selected. The result is stored as the object acc_recT, which is further converted into a vector a. With b and c, the processed answers are computed and lastly, these results are appended into three vectors Score, IDes, and try. 

accu_L2L1_L1Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L1Pro)<-c("ID", "Try", "Score")
accu_L2L1_L1Pro

# In the last part, the three vectors built before are combined into a final data frame, where each vector becomes a column in a table, each row corresponding to one participant. These columns are renamed and the table is printed. If we want to properly see the table outside of the console, we can use the View() function:

View(accu_L2L1_L1Pro)

```

```{r}
## L1 production test
# L1 → L2

# With this code chunk, we follow the same steps, but for the other learning direction (L1 -> L2 of the L1 production set).

ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L1Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L1Pro)<-c("ID", "Try", "Score")
accu_L1L2_L1Pro
```

```{r}
# L1 production: Statistics and plot

# To provide descriptive statistics, we now want to use the describe() function on the Score column, which returns a rich set of stats.

#L2 → L1 learning
describe(accu_L2L1_L1Pro$Score)

#L1 → L2 learning
describe(accu_L1L2_L1Pro$Score)

# Plot (L1 production)

# To visualize this, we want to create a boxplot comparing the two learning directions. For this to work, the beeswarm package has to be installed and loaded:

# install.packages("beeswarm")

library(beeswarm)

# For this plot, we want to assign a data frame L1pro that contains the scores from both learning directions in the L1 production test. After changing the names of the columns to how we want them to appear on the x-axis, we create a side-by-side boxplot for the learning conditions. Finally, we add the beeswarm() function specifying add=T, which lets the individual scores appear as jittered dots that don't overlap.

L1pro<-data.frame(accu_L2L1_L1Pro$Score,accu_L1L2_L1Pro$Score)
names(L1pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L1pro,col="grey91", outline = T)
beeswarm(L1pro,add=T)
```

Boxplots are a way to visualize both the central tendency (median) of a
variable and the spread around this central tendency (IQR). The median
is represented by the thick line inside the box, while the box
represents interquartile range, meaning the range of the middle 50% of
the data. The whiskers outside the box extend to the highest and
smallest values, and the jittered dots represent individual data points.
As we can see here, the median is similar in both learning directions.
L2→L1 displays slightly greater variability and a higher concentration
of upper outliers. Overall, performance appears comparable between the
two groups.

```{r}
## L2 production test
# L2 -> L1

# This chunk of code follows the same steps as before with the L1 production set, using L2 scores.

#L2→L1 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L2L1_L2Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L2Pro)<-c("ID", "Try", "Score")
accu_L2L1_L2Pro
```

```{r}
## L2 production test
# L1 -> L2

#L1→L2 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L2Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L2Pro)<-c("ID", "Try", "Score")
accu_L1L2_L2Pro
```

```{r}
# L2 production: Statistics and plot

# In this chunk of code, we want to provide descriptive statistics for L2 production test as well, and visualize them in a similar boxplot.

# L2 -> L1 learning

describe(accu_L2L1_L2Pro$Score)

# L1 -> L2 learning

describe(accu_L1L2_L2Pro$Score)

# As with L1, we create a boxplot comparing the two learning directions in L2 production using the beeswarm package.

#Plot
L2pro<-data.frame(accu_L2L1_L2Pro$Score,accu_L1L2_L2Pro$Score)
names(L2pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L2pro,col="grey91", outline = T)
beeswarm(L2pro,add=T)

```

Median scores in the L2 production test are similar for both learning
directions (L2→L1 and L1→L2), suggesting comparable central performance.
L1→L2 learning shows a slightly higher median. In the L2→L1 group,
values below the median are more spread out, indicating more variety.
Comparing L1 and L2 production tests, L1 production scores appear to be
generally higher and more consistent across learning directions.

```{r}
# Summarized results: Descriptive statistics of the tests

# In this chunk, we want to create a table that displays the summarized results of descriptive statistics, similar to the Table 4 in the paper (p. 1126). As with the target items, there was no code available for creating the tables. But with the available data, we can find a way to reproduce them anyway!

# First, we need to load the knitr and kableExtra packages.

library(knitr)
library(kableExtra)

# Next, we want to assign a variable that contains the scores for both data sets (L1 and L2 production) and use the describe() function to get all the important measures.

descriptive_stats_tests <- data.frame(L1pro, L2pro) |> 
  describe()

# In keeping with the table in the paper, the first two columns (number of observations n = 28, and vars) are removed. Also, the rows are renamed in a more readable way.

descriptive_stats_tests_trimmed <- descriptive_stats_tests |> 
  select(-n, -vars, -trimmed, -mad, -range, -se)

rownames(descriptive_stats_tests_trimmed) <- c("L2 → L1 (L1pro)", "L1 → L2 (L1pro)", "L2 → L1 (L2pro)", "L1 → L2 (L2pro)")

# Now we want to display the results in a clean, formatted table using the kable() function.

kable(descriptive_stats_tests_trimmed, 
      caption = "Descriptive Statistics for Test Scores",
      digits = 2) |> 
  pack_rows(index = c("L1 production test" = 2, "L2 production test" = 2)) |> 
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))


# Now we have a table that summarizes the descriptive statistics of the two types of post-tests, where the measures of scores in the two production tests can be easily compared. In the L1 production test, scores were shown to be generally higher than in L2 production test.

```

```{r}
# Summarized results: Alpha coefficients for L1 and L2 production tests

# In this chunk, we want to summarize results of the reliability testing and reproduce a table similar to Table 5 in the paper (p. 1126).

alpha_A_L1 <- alpha(VocabularyA_L1production[,-1], warnings=FALSE)
alpha_A_L2 <- alpha(VocabularyA_L2production[,-1], warnings=FALSE)
alpha_B_L1 <- alpha(VocabularyB_L1production[,-1], warnings=FALSE)
alpha_B_L2 <- alpha(VocabularyB_L2production[,-1], warnings=FALSE)



alpha_table <- data.frame(
  Vocabulary = c("Vocabulary A", "Vocabulary B"),
  
  Alpha_L1 = c(
    sprintf("%.2f", alpha_A_L1$total$raw_alpha),
    sprintf("%.2f", alpha_B_L1$total$raw_alpha)
  ),
  CI_L1 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L1$feldt$lower.ci, alpha_A_L1$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L1$feldt$lower.ci, alpha_B_L1$feldt$upper.ci)
  ),
  
  Alpha_L2 = c(
    sprintf("%.2f", alpha_A_L2$total$raw_alpha),
    sprintf("%.2f", alpha_B_L2$total$raw_alpha)
  ),
  CI_L2 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L2$feldt$lower.ci, alpha_A_L2$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L2$feldt$lower.ci, alpha_B_L2$feldt$upper.ci)
  ),
  
  stringsAsFactors = FALSE
)

# Rename column headers (use empty string for "Vocabulary")

colnames(alpha_table) <- c("", "Alpha", "95% CI", "Alpha", "95% CI")

# Now we want to display the results in a clean, formatted table.

alpha_table %>%
  kable(align = "lcccc", caption = "Alpha coefficients for L1 and L2 production test") %>%
  add_header_above(c(" " = 1, "L1 production" = 2, "L2 production" = 2))

# In this chunk, we calculated Cronbach’s alpha and 95% confidence intervals for two vocabulary tests (A and B) at two proficiency levels (L1 and L2), and created a summary table showing the reliability estimates and CIs for each test and level. Aside from the alpha() function, we used sprintf() to create and format strings for the table, which was then generated using the kable() function. 

```

If we compare this table about alpha coefficients with the one in the
paper (p. 1126), we notice a difference in confidence intervals,
specifically in the hundredths place. If we look at the output of
alpha(), we see that it puts out two kinds of confidence intervals:
Feldt and Duhachek. Reading up in the help file of the alpha() function,
it becomes clear that alpha.ci (used to access CIs from alpha function)
finds CIs using the Feldt procedure, which is based on the mean
covariance, while Duhachek procedure considers the variance of the
covariances. In the help file it is stated that in March, 2022, alpha.ci
was finally fixed to follow the Feldt procedure. Since the paper was
published in 2021, this might explain the deviating CIs here. If one
would like to look at Duhachek's CI instead, it can either be seen in
the output of alpha(), or one might install an earlier version of the
package. Since we want to use the current psych package for our project
and the differences don't change our outcome, we decided to stick with
Feldt's CI and simply wanted to note why this difference occurs here.

As mentioned before, Cronbach's alpha indicates whether responses are
consistent between items, and ranges between 0 and 1, a higher value
meaning higher reliability. As we can see in the results of the
reliability analysis, alpha coefficients range from .73 to .84, showing
adequate reliability of all the tests.

# Generalized linear mixed-effects models

Terai et al. (2021) used generalized linear mixed-effect models (GLMM)
to analyze and examine three variables: Learning Condition (L2 to L1 and
L1 to L2), Test Type (L1 production and L2 production), and Vocabulary
Size (L2 Proficiency), as well as interaction between two variables
(p.1125).Three models were chosen for this analysis, the first one to
analyze the relationship between learning conditions and production
tests (RQ1) (Terai et al. 2021 : 1125). The second and third model
analyzed the effects of the two learning directions (L2 to L1 and L1 to
L2) based on the results of the production tests (RQ2). They are pretty
similar, except that the third and last model also used L2 production
test scores.

Generalized linear mixed-models (GLMMs) are an extension of linear mixed
models. They are tools that help us to analyze not normally distributed
data, such as binary and count data, when observations are clustered in
some way. GLMMs are mostly used to include some models that are used
commonly, such as logistic regression or linear regression. What is
great about these models, is that they allow for analysis of clustered
or longitudinal data that is not continuous (Gurka & Edwards 2011).

MAYBE A BETTER EXPLANATION NEEDED??? BETTER SOURCE TOO

phia package (De Rosario-Martinze 2015) emmeans package (Lenth 2019)

## Effects of Learning Condition (Research Question 1)

As we recall from the introduction, the first research question of Terai
et al.'s study is: Which learning direction is more effective for
vocabulary acquisition, L1 to L2 learning or L2 to L1 learning?
Therefore, the first model was built to analyze the relationship between
the production tests and learning conditions. It contained Learning
Condition and Test Type as explanatory variables, as well as the
interaction of the two variables. Random effects (Subject and Item) were
included, and production test answers were used as the response variable
(p. 1125).

```{r}
# Before starting on the model, we install and load the packages we need. These seem to be from an older Rstudio version, but they still work just fine!

# install.packages("lme4", discrepancies = TRUE)
# install.packages("effects", discrepancies = TRUE)
# install.packages("emmeans", discrepancies = TRUE)
# install.packages("phia", discrepancies = TRUE)
library(lme4) ##leading required package:Matrix/ this package is for mixed models
library(effects) # -> package is for plotting model effects
library(emmeans) # -> package is for post-hoc comparisons
library(phia) # same as emmeans

```

### Model 1 without interaction

Now, we fit a logistic mixed-effects regression model predicting
Accuracy (Answer) from Test, Condition, and their interaction. We want
to go through the process step by step, and connect the code with the
interpretation of data in the paper.

```{r}
# First, we fit the model without interaction, using the glmer() function.

# Model without interaction
fit1<-glmer(Answer~Test+Condition+(1|ID)+(1|ItemID),family=binomial, data=dat, glmerControl(optimizer="bobyqa"))

summary(fit1)

# With the glmer() function, we fit a GLLM. In the formula of this function, both fixed effects (Test answers and Condition) and random effects (ID and ItemID) are specified. (1 | ID) allows for a random intercept for each level of the variable ID (participants), (1 | ItemID) does the same for each level of the variable ItemID (test items). This accounts for repeated measures and inter-individual and inter-item variability. With specifying family=binomial, we tell the glmer() function that our response variable is binary (correct answer = 1, incorrect answer = 0). The last argument specifies an optimizing algorithm. We use the summarize() function to look at the output of this model.

# Setting number of significant digits and calculating AIC
options(digits=7)
AIC(fit1)
```

Without interaction, the model assumes that the effects of Test and
Condition are additive and do not depend on each other. AIC (Akike
Information Criterion) was calculated for both variants of the model. It
is used to estimate the quality of a model in comparison to another one,
with a lower AIC indicating higher quality. Therefore, we will get back
to this after fitting the model with interaction, and compare the two.

If we look at the output of the summary() function for our model, the
output includes *scaled residuals*. Residuals are the differences
between observed and predicted values. This section shows summary
statistics of the model's residuals after scaling: Minimum, first
quartile (25^th^ percentile), median (50^th^ percentile), third quartile
(75^th^ percentile), and maximum. These values give an impression of how
well the model fits the data. The *Random effects* section puts out the
estimated variability in the intercept across subjects (ID) and items
(ItemID), accounting for differences in overall accuracy between
participants and between items.

### Model 1 with interaction

Next, we fit a model similar to the first one, but with the difference
that it includes the interaction between Test and Condition,
investigating if the effect of one variable depends on the level of the
other variable.

```{r}
# For the model with interaction, we use glmer() as well, but with a change in the formula (connect the effects with * instead of +).

# Model with interaction
fit1.1<-glmer(Answer~Test*Condition+(1|ID)+(1|ItemID),family=binomial, data=dat, glmerControl(optimizer="bobyqa"))
summary(fit1.1)

# Computing confidence intervals

confint(fit1.1)

# Setting number of significant digits and calculating AIC

options(digits=7)
AIC(fit1.1)
```

Now after calculating AICs for both versions of the model, we can see
that the model *with* interaction showed lower AIC (with interaction:
2427.102; without interaction: 2429.771), indicating higher quality of
the model.

The authors state that the results show a significant main effect of
**Test Type** (Estimate = -0.976, SE = 0.105, z = -9.315, p \< .001). We
want to find these results in the output of our code and see how they
lead to this interpretation. If we look at the summary output of fit1.1
under Fixed effects, the numbers can be found in the "Test2" row. They
are all rounded up to three digits for better readability. The negative
estimate tells us that accuracy in Test 2 is lower than in Test 1. The
p-value can be extracted from the column marked Pr(\>\|z\|), where it is
displayed in the form of \<2e-16, since it is such a small number. It
tells us that the effect (of switching between test types) is
significant. The z value tells us how many standard deviations away a
value is from the mean.

Moving on to **Learning Condition**, there was no main effect, as we can
see under "Condition2" (Estimate = -0.038, SE = 0.103, z = -0.366, p =
.714). In this case, the p-value is \> 0.05, meaning that the effect of
learning condition is *not* significant. The main effect estimate of
Condition2 is quite small (close to 0), telling us the predicted odds
don't change much when moving from one learning condition to the other.
The **interaction** of Test Type and Learning Condition (see row
"Test2:Condition2") was also significant (Estimate = -0.446, SE = 0.206,
z = -2.169, p = .030). The interaction term tells us how the effect of
Test on accuracy changes depends on the Condition, or the other way
around. The estimate tells us that, when both Test2 and Condition2 are
present, the accuracy is even lower than when only one of them is. The
p-value of .030 indicates that the interaction effect is unlikely due to
chance.

### Model comparison

We want to move on to further analyze and compare the two models. 
```{r}
# In this chunk, we want to compare the model with and without interaction, using the anova() function:

anova(fit1,fit1.1)

# Checking simple main effects in both directions with testInteractions():

testInteractions(fit1.1, fixed="Test", across="Condition")

testInteractions(fit1.1, fixed="Condition", across="Test")
```

Since a significant effect of the interaction was found, we compared the
two models with the anova() function to test if the interaction improves
the model. It puts out a p-value of 0.03071. Being smaller than 0.05,
the effect is statistically significant, which is also symbolized by the
\* specified in the significance codes in the output. It tells us that
fit1.1 (model with interaction) fits the data significantly better than
fit1 (without interaction). With the testInteractions() function, it is
possible to perform simple main effects analysis after finding a
significant interaction. A significant interaction only tells you that
the effect of one variable depends on the level of the other, not which
levels are different. So with this function, we want to unpack the
interaction further and find out how the effect of one variable (Test or
Condition) differs across levels of the other. If we look at the output
of the first function where **Test** is used as the fixed variable, the
p-values (L1 Production: 0.188; L2 Production: 0.164) tell us that the
effect of Test does *not* significantly differ between conditions. If we
take **Condition** as the fixed variable, p-values are very small (L1L2:
1.931e-07; L2L1: 1.608e-15), indicating that the effect of Condition
*does* significantly differ between tests.

### Multiple comparisons

After comparing the models and testing the interaction, we want to conduct pairwise comparisons
on the effects of Condition and Test.

```{r}
# Through computing estimated marginal means using the emmeans() function, we want to conduct pairwise comparisons of Condition levels within each level of Test, and assign it to a variable a.

a<-emmeans(fit1.1, pairwise~Condition|Test, adjust="tukey")
  a
  
# Computing confidence intervals for emmeans object a
confint(a,parm,level=0.95)

# Calculating effect sizes
eff_size(a,sigma=sigma(fit1.1), edf= Inf)

# The same procedure is now applied in the other direction: While with variable a, we told emmeans() to do pairwise comparisons of Condition levels within each level of Test, we now want all comparisons of Test levels within each level of Condition, and save this object as the variable b.

# Computing EMMs 
b<-emmeans(fit1.1, pairwise~Test|Condition, adjust="tukey")
  b
  
# Computing confidence intervals for emmeans object b
confint(b,parm,level=0.95)

# Calculating effect sizes
eff_size(b,sigma=sigma(fit1.1), edf= Inf)

```

The analysis conducted in the code chunk above provides the most
important information for the interpretation of this model done by the
authors. We want to go through their statements and connect them with
the code we reproduced. In the paper, the authors claim there was a
statistically significant difference between the scores of the two
tests, suggesting that L1 production test scores were higher than L2
production test scores in both learning conditions (**L2 to L1
learning:** p \< .001, d = 1.20, 95% CI \[0.91, 1.49\]; **L1 to L2
learning:** p \< .001, d = 0.75, \[0.47, 1.04\]) (p. 1126). Now, we want
to take a close look at these numbers and what they mean. In our code,
we can find the numbers stated here in the output regarding the emmeans
object b. The p-values show up when printing b, and lie \< .001 for both
learning conditions. A small p-value tells us that an effect likely
exists, but does not say much about how big or important it is. This is
where effect size comes into play: The function eff_size() puts out the
estimated effect sizes for both conditions, which are referred to as
**d** in the paper. They both demonstrate a large (larger for L2 to L1)
effect size, and indicate that the difference (different levels of Test
within each Condition) is not just statistically significant, but
meaningful in size. Lastly, the confint() function applied to b put out
the 95% confidence intervals for both learning directions, which means
we can be 95% confident that the true standardized effect size lies in
this range (which is a meaningful effect size, in our case).

It is further expressed that results showed no main effects of Learning
Condition (L1 production test: p = .188, d = -0.19, 95% CI \[-0.46,
0.09\]; L2 production test: p = .082, d = 0.26, \[-0.03, 0.55\]). This
can be derived when we look at object a, which compares learning
conditions separately for L1 Production and L2 Production. Printing a
puts out p-values, which lie above 0.05 in both production tests and
indicate no statistical significance of the differences between learning
conditions. The effect sizes show a small negative effect for L1
Production, and a small positive effect for L2 Production, and do not
suggest a meaningful difference. The confidence intervals in both cases
include 0 (since it lies between -0.03 and 0.55, for example), telling
us that no effect (d = 0) is a possibility given the data. To sum up,
all these values do *not* support a main effect of Learning Condition.

### Visualizing Model 1

Finally, we want to present the results of this model in a plot that
visualizes all important information regarding our research question.

```{r}
# Visualizing the first model

# Setting graphical parameters for the plot
par(pty="s") # sets the shape: square
par(pin=c(10,10)) # sets the plot size
par("ps") # font size

# Plot
plot(allEffects(fit1.1),multiline=T,ci.style="bands", xlab="Test Type", ylab="Accuracy Rate",
       main= "Test & Learning Direction", lty=c(1,2), rescale.axis=F, ylim = c(0,1),
       colors="black",asp=1)
```

Lastly, the model is visualized. The plot shows the interaction between
Test Type and Learning Condition on accuracy rates, based on the
generalized linear mixed-effects model (fit1.1). It is an effect plot
generated with the allEffects() function. The solid line represents the
L1 to L2 learning condition; the dotted line represents the L2 to L1
learning condition. The y-axis represents accuracy rates on both L1 and
L2 production tests. The x-axis represents Test Type. We can see that
the accuracy rates differ across Test Types and Learning Conditions,
with a clear interaction between the two: The non-parallel lines
indicate that the effect of one variable changes depending on the level
of the other variable. The distance between the two lines is relatively
small at each test type, which reflects the results that revealed no
significant difference in learning effects between L2 to L1 learning and
L1 to L2 learning.

### Extension (Hannah)

To extend the work on this model, I want to plot the results using the
ggplot() function instead. I will explain later why this could be of
advantage.

```{r}
# Visualizing with ggplot2

# First, we need to load lme4, gglpot2 and paletteer libraries:
library(lme4)
library(ggplot2)
library(paletteer)

# I want to present the values of b$emmeans, which contains estimated marginal means. ggplot works with data frames, not with glm objects. If we check the structure of our object, it returns an emmGrid object:

str(b$emmeans)

# To create a ggplot, we need to transform the EMMs back to the original scale of the response variable with specifiying the type, and then convert our emmeans object into a data frame:

b <- emmeans(fit1.1, pairwise ~ Test | Condition, adjust = "tukey", type = "response")

emm_df <- as.data.frame(b$emmeans)

# Plot using ggplot2
ggplot(emm_df, aes(x = Test, y = prob, group = Condition, color = Condition, linetype = Condition)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL), width = 0.1) +
  scale_color_paletteer_d("beyonce::X11") +
  scale_linetype_manual(values = c("solid", "dashed")) +
  scale_y_continuous(name = "Accuracy Rate", limits = c(0.0, 0.75)) +
  xlab("Test Type") +
  ggtitle("Test & Learning Direction") +
  theme_minimal() +
  theme(
    text = element_text(size = 14),
    plot.title = element_text(hjust = 0.5)
  )

```

This plot is quite similar to the one presented in the author's code.
The only addition in *what* is presented, is that it contains so-called
error bars above and below each point, which reflect the confidence
intervals. Most differences lie in *how* the data is presented, where I
changed the appearance a bit. I added the Beyonce palette, to give the
two lines (that represent Learning Condition) some color. I still kept
them solid vs. dashed, so that they can be told apart easily. Also, the
legend for "Condition" is next to the plot, not above it, which I find
more well-arranged.

While this reproduces the plot that the authors already provided in
their code, I want to explain why using ggplot2 for this might be of
advantage. ggplot2 is built on the "grammar of graphics", which makes it
easy to add layers and extend the plot. There are many options to alter
the appearance of the plot, like colors, themes etc. Furthermore, in
this code, we can explicitly see what is plotted and how specific
elements are modified. This makes the plot more reproducible and easier
to update or change later on.

## Effects of Learning Directions and L2 Proficiency (RQ2)

Terai et al. (2021) hypothesized that the effectiveness of learning
direction or word pairs might be dependant on the learners' proficiency
in English. Meaning, lower-proficiency learners might benefit more from
practicing in the L2 -\> L1 direction, whilst higher-proficiency
learners might benefit more from the L1 -\> L2 direction.

To answer and test this question, the authors once again use generalized
linear mixed models (GLMMs) separately for the L1 and L2 production
tests, with Condition (learning direction), scaled vocabulary size
(s.vocab), and their interaction as fixed effects, and Subject/Item
random intercepts.

Before we work through and understand the code step by step like the
author's did, we need to know what we are working with here: We have a
response variable which in our case is the accuracy on production tests
and predictor variables which were learning direction (L1 -\> L2 vs. L2
-\> L1) and vocabulary size (English proficiency). Our random effects
were Participant ID and Item ID so that we can account for individual
differences.

### L1 Production test

We begin by focusing only on the L1 production data. This can be done by
first loading our data with the command read.csv() and then the command
filter() which only keeps the rows for the L1 Production test. We filter
this data because while each participant took both L1 and L2 production
tests, the authors of this study found different patterns for both test
types which is why they chose to analyze them separately (p. 1126). This
is the part where participants of the study had to produce the L1
(Japanese) word when shown the L2 (English) word.

```{r}
dat1 <- read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header = TRUE)

dat1.L1<-filter(dat1, Test=="L1 Production")

```

To check whether you understood the filter() function completely, use
the 'nrow()' function to find out! How many rows were in the original
dataset compared to the filtered dataset?

```{r}
nrow(dat1)
nrow(dat1.L1)
```

Next, we want to declare the learning direction 'Condition' as a factor.
Here we need to tell R that 'Condition' represents categories and not
numbers. This is why we convert the categories (L1 -\> L2 vs. L2 -\> L1)
into factors, that way R will treat it as a categorical variable.
Converting our variables into factors now will also be useful for any
other statistical analysis such as an ANOVA which we will be doing in a
later step.

```{r}
#Very basic: We set Condition (Learning direction) as a factor
dat1.L1$Condition <- as.factor(dat1.L1$Condition)
```

We make use of contrast coding once more:

```{r}

c <- contr.treatment(2)
my.coding <- matrix(rep(1/2,2,ncol=1))
my.simple <- c-my.coding
print(my.simple)

contrasts(dat1.L1$Condition) <- my.simple

```

In the next step, we want to scale the continuous predictor. This
converts the vocabulary scores to have a mean of 0 and standard
deviation of 1 which is helpful for interpretation!

```{r}

dat1.L1$s.vocab <- scale(dat1.L1$Vocab)

```

### Model without interaction

This model without interaction assumes that learning direction and L2
proficiency each have independent effects on test accuracy.

```{r}
#Answer~Condition+s.vocab: includes main effects of learning condition and proficiency plus their interaction
#(1|ID) + (1|ItemID): random intercepts for participants and items (to account for repeated measures)
#family = binominal: because our outcome is binary (correct/incorrect)

fit2<-glmer(Answer ~ s.vocab + Condition + (1|ID) + (1|ItemID),
            family = binomial, 
            data = dat1.L1, 
            glmerControl(optimizer="bobyqa"))

summary(fit2)

```

```{r}
options(digits=7)
AIC(fit2) 
##This command prints AIC, a fit index (the lower the better)
```

The model predicts the probability of a correct response (Answer), using
logistic regression. The random intercepts (1\|ID) and (1\|ItemID)
account for individual differences between participants and between
vocabulary items. The output of this model without interaction shows
that s.vocab is not significant with p = .57, this means that
proficiency by itself did not strongly predict L1 Production accuracy.
For 'Condition' the model also showed that it was not significant p =
.18, meaning that learning direction alone also did not explain
differences. So if we look at each predictor separately, there seems to
be no clear effect.

### Model with Interaction

What if proficiency only matters depending on the learning direction? To
figure this out, we add an interaction term: s.vocab \* Condition.

```{r}
fit2.1<-glmer(Answer ~ s.vocab * Condition + (1|ID) + (1|ItemID),
              family = binomial, 
              data = dat1.L1, 
              glmerControl(optimizer="bobyqa"))

summary(fit2.1)
confint(fit2.1)
options(digits=7)
AIC(fit2.1)
```

The main difference in this model is that s.vocab \* Condition here
includes both main effects and their interaction. We tested whether the
effect of vocabulary differs between the learning conditions. Just as we
read in the paper, the interaction of Learning Condition and Vocabulary
Size is significant with an estimate of about -0.37, SE = 0.141, z =
-2.596, p = .009, which means that the effect of vocabulary proficiency
depends on the learning direction. There were no significant effects of
Vocabulary Size or learning condition by themselves.

### Model comparison

To formally check which model fits better using AIC and a likelihood
ratio, we use the command anova(), which is an analysis of variance (a
statistical formula which compares variances across the means (or
average) of different groups).

```{r}
anova(fit2,fit2.1)
```

The AIC favors the interaction model (1311.6 vs. 1316.3), and the
likelihood ratio is statistically significant (χ² = 6.70, p = .009),
indicating and confirming that the model with the Condition x
Proficiency interaction fits best.

### Visualisation

```{r}
plot(allEffects(fit2.1), multiline=T, ci.style="bands",
    xlab="Vocabulary Size", ylab="Accuracy Rate",
    main="L1 Production", lty=c(1,2),
    rescale.axis=F, ylim = c(0,1), colors="black", asp=1)
```

Looking at the plot you see a nearly flat line for L2 -\> L1 and a
positive, steeper line for L1 -\> L2 which cross at intermediate
proficiency (\~5.419 as reported in the paper, p. 1127). This plot shows
that accuracy increases with proficiency. The slope differs by
condition. It is much steeper for L1 -\> L2. For lower proficiency
learners, L2 -\> L1 is better, and for higher proficiency learners L1
-\> L2 is more beneficial. Learners in one learning direction seem to
benefit more strongly from larger vocabularies.

### Testing simple effects

You can also test whether the slope of proficiency differs significantly
between conditions as predicted by the authors. This is very basically a
Chi-square test.

```{r}
testInteractions(fit2.1, fixed="s.vocab", across="Condition")
```

## L2 Production Test

We now repeat all the same steps for the L2 production test, but now we
predict accuracy for when the participants had to produce English words
after learning Japanese-English pairs.

```{r}
dat2 <- read.csv(file = here("data", "dataset1_ssla_20210313.csv"))

dat2.L2 <- filter(dat2, Test == "L2 Production")

dat2.L2$Condition <- as.factor(dat2.L2$Condition)

c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)
```

```{r}
contrasts(dat2.L2$Condition) <- my.simple 

dat2.L2$s.vocab <- scale(dat2.L2$Vocab)


fit3 <- glmer(Answer ~ s.vocab+Condition + (1|ID) + (1|ItemID),
              family=binomial, 
              data=dat2.L2, 
              glmerControl(optimizer="bobyqa"))
summary(fit3)
```

```{r}
options(digits=7)
AIC(fit3)
```

```{r}
fit3.1 <- glmer(Answer ~ s.vocab*Condition + (1|ID) + (1|ItemID),
                family=binomial,
                data=dat2.L2, 
                glmerControl(optimizer="bobyqa"))
summary(fit3.1)

confint(fit3.1)

options(digits=7)
AIC(fit3.1)
```

### Model Comparison

To formally check whether the model with the interaction is better than
the simpler model without the interaction, we compare them with test:

```{r}
anova(fit3,fit3.1)
```

### Visualisation

```{r}
plot(allEffects(fit3.1),multiline=T,ci.style="bands", xlab="Vocabulary Size", ylab="Accuracy Rate",
       main= "L2 Production", lty=c(1,2), rescale.axis=F, ylim = c(0,1),colors="black",asp=1)
```

Interestingly, while there was no significance of the results for
learning direction or L2 proficiency (Estimate -0.278, SE = 0.154, z =
-1.810 p = .070). We can see in the plot that the trend seems similar to
the results of our L1 production test (flatter L2 -\> L1 slope, steeper
L1 -\> L2 slope). So, for lower-proficiency learners, L2 -\> L1 leads to
higher performance than L1 -\> L2 learning. Furthermore, something that
the authors explained (p. 1128) was that we can see clearly in our
graphs by the nearly flat dashed line, that L2 -\> L1 learning is much
less influenced by L2 proficiency

### Interpretation & Take-home message

Generally, L1 production showed that lower proficiency learners
benefitted from L2 -\> L1 learning direction, while higher proficiency
speakers benefitted from L1 -\> L2 learning direction. For the L2
production similar things were observed but were not statistically
significant. Vocabulary proficiency shapes how effectively retrieval
practice works, and the optimal learning direction may differ for
learners with higher vs. lower proficiency. The optimal learning
direction is clearly not a one-size-fits-all matter, it depends on
learner proficiency.

What you learned in this section: - Adding a continous predictor
(s.vocab) to a GLMM. - Testing and interpreting interactions. -
Comparing models using AIC and likelihood ratio tests. - Visualizing
interaction effects with the effects package.

## Model Diagnostics

Now, we checked our GLMMs and have our results. But before interpreting
these results too confidently, we should also check whether the
predictors are highly correlated with one another which is a problem
called multicollinearity. Multicollinearity might affect the results of
our GLMMs.

Why does this matter?

-   If our predictors are strongly correlated, the model has trouble
    keeping apart their unique contributions.
-   This inflates standard errors, making estimates unreliable.

A common diagnostic here is the Variance Inflation Factor (VIF). A VIF
close to 1 means predictors are not collinear. Rules: -\> VIF = 1:
perfect independence -\> VIF \> 5: potential concern -\> VIF \> 10:
serious multicollinearity

### Calculating VIFs for the Models

We use a helper function vif.mer() to calculate VIFs for mixed models.

```{r}
source("https://raw.githubusercontent.com/aufrank/R-hacks/master/mer-utils.R")

vif.mer(fit1.1)

vif.mer(fit2.1)

vif.mer(fit3.1)
```

All our VIF values are obviously quite close to 1. This means that the
predictors (s.vocab, Condition, and their interaction) are not
correlated with each other in problematic ways. Therefore our models are
statistically stable: the estimate of the interaction effects we
observed in our GLMMS are not due to multicollinearity.

We can confidently say that the interaction between learning direction
and proficiency is a genuine effect, not an artifact of overlapping
predictors. The models are behaving well in terms of collinearity.

# Conclusion

You are now familiar with handling generalized linear mixed models! You
have learned how to fit them, interpret their main effects and
interactions, as well as checking their assumptions with VIFs. GLMMs are
powerful tools for analyzing binary outcomes such as accuracy
(SOURCE??), which allows us to account for repeated measures across
participants and items. It is completely normal that at times GLMMs can
feel intimidating: coding contrasts and interpreting the outcomes are
not always straightforward. But, similar to plotting, there is always a
solution. The insights we can gain from a nicely fitted model make all
the effort worthwhile.

This chapter's analysis revealed that the effect of the learning
direction is not fixed, but rather depends on L2 proficiency. In the L1
Production Test, learners with a lower-proficiency appeared to benefit
more from L1 to L2 practice, while learners with a higher-proficiency
showed the opposite pattern. In the L2 Production Test we observed a
similar trend, though the interaction was weaker. These results suggest
that the learning direction may shift as learners' vocabulary knowledge
grows.

In this chapter, we combined descriptive statistics, mixed-effects
modeling, and visualizations to answer the study's research question.
The answer is nuanced: both learning directions can be effective, but
their benefits change from proficiency level to proficiency level. While
working on this, we also figured out how model comparison (AIC) and
diagnostics (VIFs) helped ensure that our conclusions are statistically
right.

It seems that vocabulary learning is not something that can be
generalized. While retrieval practice is quite effective, the direction
of practice depends on who the learner is and how much this learner
knows of that language. Due to the extensive research by Terai,
Yamashita and Pasich (2021) who published their data set and code along
with the paper, we were able to reproduce their findings and gain
hands-on experience with modeling. This not only helps with our
understanding of the research, but it also sharpens our skills as
analysts of second-language data.

# Sources

## Packages used in this chapter

```{r}
#| echo: false

print(sessionInfo())

```

