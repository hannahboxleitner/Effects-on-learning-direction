---
title: "Reproducibility study: Effects of Learning Direction in Retrieval Practice on EFL Vocabulary Learning"
subtitle: "Based on data from Terai et al. (2021)"
authors: "Hannah Boxleitner, Beyhan Aliy"
date: last-modified
format:
  html:
    number-sections: true
    embed-resources: true
    fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
---

# Chapter overview

This chapter is based on data published in:

-   Terai, Masato, Junko Yamashita, and Kelly E. Pasich. 2021. EFFECTS
    OF LEARNING DIRECTION IN RETRIEVAL PRACTICE ON EFL VOCABULARY
    LEARNING. Studies in Second Language Acquisition 43(5): 1116–1137.
    <https://doi.org/10.1017/S0272263121000346>.

This chapter will introduce the application of R in Second Language
Acquisition research, using data from Terai et al. (2021). We will go
through the entire data analysis pipeline, from importing the data
(chapter ???) to running logistic regression models and visualizing
results with it. By the end of this chapter, it should become clear how
to conduct and interpret mixed-effects logistics regression models in R
for experimental data.


# Introducing the study

In this chapter, we attempt to reproduce the results of a linguistics
study by Terai et al. (2021). The study focuses on learning directions
in pair-associated vocabulary learning: L2 to L1, where L2 words are
used as stimuli and responses are given in L1, vs. L1 to L2. Opposed to
studying practices, where both a target and the answer are
simultaneously presented, pair-associated learning includes *retrieval*.
Retrieval is defined as the process of accessing stored information and
plays a crucial role in retaining a learned word in memory **(p.
1116-1117 - Literaturangabe?)**. Previous findings regarding the
efficacy of the two types of learning directions are inconsistent. The
study focuses on the relationship between L2 proficiency and the
effectiveness of the two learning directions in paired-associate
learning in L2 vocabulary acquisition, and follows two research
questions: 1. Which learning direction is more effective for vocabulary
acquisition, L1 to L2 learning or L2 to L1 learning? 2. How does L2
proficiency influence the effects of L2 to L1 and L1 to L2 learning?

To answer these questions, the authors designed an experiment where Japanese EFL learners studied word pairs and then completed retrieval practive tasks in different conditions. After learning, students were tested on their ability to produce the target vocabulary items both in L1 and L2.

Therefore, the authors established the hypotheses that there is no
significant difference between the two learning directions and that the
effect of learning direction depends on the learner’s L2 proficiency
**(p. 1121-1122).**

In this chapter, we will:

- load and explore the dataset made available by the authors,
- prepare the variables for analysis,
- run generalized linear mixed models (GLMMs) to answer the two research questions,
- and learn how to interpret and visualize the results.

Our aim in this chapter is not only to replicate the study by Terai et al. (2021) but also to learn how to apply mixed-effect modeling in R to real research questions in second language acquisition. 

# Retrieving the authors' data

We will use the authors' original data for our reproduction, which they
have made openly accessible on IRIS, a free database for language
research:

-   [Terai, M., Yamashita, J. & Pasich, K. E. (2021). Effects of
    Learning Direction in Retrieval Practice on EFL Vocabulary Learning.
    Studies in Second Language Acquisition. 43(5).
    1116–1137)](https://www.iris-database.org/search/?s_publicationAPAInlineReference=Terai%20et%20al.%20(2021))

In this particular case, you cannot just access the data, but also the
authors' R code. Our further reproduction will use this R code to
investigate and interpret the statistical analysis and results presented
in the paper.

The dataset (dataset1_ssla_20210313.csv) contains data for each
participant and item. Each row includes:

-   Answer: whether the participant gave the correct answer (1 =
    correct, 0 = incorrect)

-   Test: test type (L1 Production or L2 Production)

-   Condition: learning direction (L2-\>L1 or L1-\>L2)

-   ID: participant identifier

-   ItemID: item (word) identifier

-   Vocab: vocabulary test score (L2 proficiency)

-   *more ( L2 frequency?, length, syllables, etc.?)? -\> why not
    included?*

# Importing and setting up the data

Before importing the dataset and starting on our project, we need to
load all the packages that we will need. Some of these packages may have
to installed first **(maybe include textbook reference 13.10)**.

```{r}
#| label: setup
#| warning: false

# This chunk loads necessary libraries.

# Load libraries
library(here) 
library(tidyverse)

```

Next, we can import the data. In contrast to the authors' approach in
their code, we used the here package for importing the data. This
package creates paths relative to the top-level directory and therefore
makes it easier to reference files regardless of how they are organized
inside a project.

```{r}
VocabularyA_L1production <- read.csv(file = here("data", "vocaA_L1pro_20210313.csv"))
VocabularyA_L2production <- read.csv(file = here("data", "vocaA_L2pro_20210313.csv"))
VocabularyB_L1production <- read.csv(file = here("data", "vocaB_L1pro_20210313.csv"))
VocabularyB_L2production <- read.csv(file = here("data", "vocaB_L2pro_20210313.csv"))

#First, we will load the dataset
dat<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

#Then, we need to convert the categorical variables we have to factors using the function as.factor()
dat$Test<-as.factor(dat$Test)
dat$Condition<-as.factor(dat$Condition)

#Next, we will contrast the coding (-0.5 vs. +0.5) for categorical predictors.
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)

contrasts(dat$Test)<-my.simple
contrasts(dat$Condition)<-my.simple

#And standardize vocabulary scores (centered and scaled)
dat$s.vocab<-scale(dat$Vocab)

##All these steps set up the dataset for modeling. Converting variables to factors and coding them with contrast ensures that they are interpreted correctly in the regression. Scaling the vocabulary scores helps with interaction terms.
```

# Descriptive statistics

Before we dive into the descriptive statistics conducted in this study, we need to explain that one part of it will not be reproduced. While descriptive statistics about the participants (age, years of learning etc.) are mentioned in the paper by Terai et al. (2021), the data to reproduce these findings cannot be found. It is neither part of the data set accessible on IRIS, nor does it come up anywhere in the authors' R code. Therefore, we will only focus on the descriptive statistics of the target items (40 English words used in the study), which is Table 3 in Terai et al. (2021).

Most of the statistics that we will be looking at are things we are already familiar with: the mean, median, maximum and minimum, as well as the standard deviation (SD). In addition, the authors also reported skewness and kurtosis. 

Skewness and Kurtosis are both measures of shapes and distribution of a dataset in qualitative methods. Skewness measures the asymmetry of distribution. Simply put, it describes the amount of which the distribution differs from a completely symmetric shape. 

Skew = 0 -> perfectly symmetrical.
Negative Skew -> longer "tail" on the left.
Positive Skew -> longer "tail" on the right.

Kurtosis then is a measure of peaks of a distribution. Basically, it is a description of how the peaks compare to a normal distribution. 

Kurtosis = 0 -> similar to a normal distribution.
Positive kurtosis -> sharper peak (more concentrated in the center).
Negative kurtosis -> flatter peak. 


# Table 3: Descriptive Statistics of target items

We want to reproduce Table 3 in Terai et al. (2021) which shows us the descriptive statistics of the target items of the study. Unfortunately the code for this table was not included in the R code published by the authors. That is not an issue though because we have the data in the form of a csv file! This is great for us. We have the organized data ready for us to analyze. Thus, we will learn how to reproduce the data of Table 3 step by step.

We have already loaded the data we will need for our table, but just to be sure, we will do it again and also load all the necessary libraries for the two tables, so we have a kind of "toolbox" that contains all the functions we might need:

```{r}
# Install these once if you do not already have them
# install.packages("readr")       -> reading data
# install.packages("dplyr")       -> data wrangling
# install.packages("moments")     -> skewness and kurtosis
# install.packages("knitr")       -> for pretty tables
# install.packages("kableExtra")  -> for even prettier tables

# Load packages
library(readr)      # read_csv()
library(dplyr)      # data wrangling
library(moments)    # skewness(), kurtosis()
library(knitr)      # kable()
library(kableExtra) # nicer tables


# Load the dataset (provided in the IRIS repository)
dat<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

# Clean stray spaces from ItemID (sometimes "berth " instead of "berth")
dat$ItemID <- trimws(dat$ItemID)

```

In the paper by Terai et al. (2021) we can see that the variables used are, for L2-related variables: Frequency, Syllables, and Letters. For L1-related variables: Frequency, Letters, and Mora (syllables) as well as Familiarity (Fami B). The Fami A and Frequency for L1-related variables that we see in the table are the familiarity ratings and frequencies taken from another study by Amano & Kondo (1999).

First, we need to take a look at the data the authors have uploaded for us. The column names may differ from what we want to see in the table later on which is why we briefly want to check them by having them printed:

```{r}
names(dat)
##This is what the columns in our dataset mean:
#ItemID: the English target word (40 items)
#L2Frequency is simply the frequency of the word in English (COCA: Corpus of Contemporary American English)
#Syllables: number of syllables in the English word
#Alphabet: number of letters in the English word (this is "Letters" in the L2 table)
#L1Frequency: frequency of the word in Japanese (Amano & Kondo 1999)
#WordLength: number of letters in the Japanese orthographic form
#mora: number of mora (Japanese syllable-like units)
#Familiarity: familiarity ratings from the current study (Fami (B))

```

The raw dataset has multiple rows per word because each participant contributed responses. But for descriptive statistics of the items, we only want one row per word. So we take the first value for fixed properties such as frequenc or length which are always the same and then compute the mean familiarity across the participants.


```{r}

# Make it item-level (1 row per word)
# We keep the first observed value for fixed properties
# and compute the mean familiarity across participants for each item.
items <- dat %>%
  group_by(ItemID) %>%
  summarise(
    L2Frequency = first(L2Frequency),
    Syllables   = first(Syllables),
    Alphabet    = first(Alphabet),
    L1Frequency = first(L1Frequency),
    WordLength  = first(WordLength),
    mora        = first(mora),
    familiarity = mean(familiarity, na.rm = TRUE),
    .groups = "drop"
  )
```

A quick check before we continue, we expect 40 unique items so this command should print 40: 

```{r}
nrow(items)
```

In order to avoid repeating code, we use a small function that computes all the statistics we need for one variable.

We create our own function which we will name desc(). It is simply like a small machine, we give this machine a variable and it produces an output for us, a set of summary numbers. The functions mean(), sd(), median(), min(), max() are standard R functions you are already familiar with. The package "moments" allows us to use the functions skewness() and kurtosis(). With the addition of na.rm = TRUE we tell our helper to ignore missing values. Then, to combine all these numbers we use data.frame() which makes it possible for us to print them neatly. 

```{r}
# A tiny helper to compute stats
desc <- function(x) {
  data.frame(
    Mean     = mean(x, na.rm = TRUE),
    SD       = sd(x, na.rm = TRUE),
    Median   = median(x, na.rm = TRUE),
    Minimum  = min(x, na.rm = TRUE),
    Maximum  = max(x, na.rm = TRUE),
    # Correct definitions for Table 3:
    Skew     = skewness(x, na.rm = TRUE),
    Kurtosis = kurtosis(x, na.rm = TRUE)
  )
}
```

Now that we have most of what we need for the table, we can start to build them up! There are two subtables we need.

```{r}
## Here we build our first table (tab_L2) for the L2-related variables. desc(item$L2Frequency) runs our helper function on the frequency values and so on. The command cbind(Variable = "Frequency"...) attaches a label so we know which row is which. We repeat this for syllables and letters as well and then use the command rbind() which binds all three pieces into one table. We do the same for the L1 table (tab_L1).
tab_L2 <- rbind(
  cbind(Variable = "Frequency", 
desc(items$L2Frequency)),
  cbind(Variable = "Syllables",
desc(items$Syllables)),
  cbind(Variable = "Letters",  
desc(items$Alphabet))
)

# L1-related variables
tab_L1 <- rbind(
  cbind(Variable = "Frequency",
desc(items$L1Frequency)),
  cbind(Variable = "Letters",
desc(items$WordLength)),
  cbind(Variable = "Mora (syllables)",                desc(items$mora)),
  cbind(Variable = "Fami (B)",                 desc(items$familiarity))
)
```

Great! You are almost done building the table for the descriptive statistics. In the next step, just to make the table more comprehensible, we will round the numbers to two decimals. 

```{r}
#Oftentimes numbers produced by R have many decimals we do not need, it makes tables harder to read. So we use the mutate() function which is used to change or add columns. across(where(is.numeric), ...) means thhat we apply the next step to all numeric columns. And then ~round(., 2) rounds each number to 2 decimal places. The dot (.) is just a placeholder that basically means "whatever column we are looking at currently."
tab_L2 <- tab_L2 %>% mutate(across(where(is.numeric), ~round(., 2)))
tab_L1 <- tab_L1 %>% mutate(across(where(is.numeric), ~round(., 2)))
```

Perfect, the next and easiest part of building up our two subtables is captioning them and then finally printingthem. Here we need the kable() and the kable_classic() function.

EXPLAIN

```{r}
kable(tab_L2, caption = "Table 3a. Descriptive statistics for L2 (English) item properties.",
      align = "lrrrrrr") |>
  kable_classic(full_width = FALSE)

kable(tab_L1, caption = "Table 3b. Descriptive statistics for L1 (Japanese) item properties.",
      align = "lrrrrrr") |>
  kable_classic(full_width = FALSE)
```

Our reproduced tables now closely match Table 3 in Terai et al. (2021)! The L2 frequency has a mean of about 1025 with a quite lare SD (~852), which shows us that some English words were quite frequent while others were much rarer. The skewness here is positive (~1.51), although different than the one reported in the paper (~1.57). The difference is only small and still shows that the distribution has a "long tail" of very high-frequency words. Syllables in English have an average mean of 2, and range from 1 to 5 with only a slight positive skewness which means that most words are short and only a few are longer, again there is a small difference in skewness. Letters average around 6.2 again with a positive skewness. While the numbers of skewness only differ marginally there are larger discrepancies for the kurtosis numbers. There are several different possibilities as to why this is the case for our reproduction of Table 3. Kurtosis is a statistic that is not calculated in the same way across all R packages. There are different definitions and reports of kurtosis which means it is quite normal for the values to differ slightly depending on the method. In our reproduction we use the "moments" package, which calculates excess kurtosis, so mismatches with the published paper are expected. (SOUURCE)

In the L1 table, the frequency distribution also shows a strong positive skewness which means that a few Japanese translations are very common but many are quite rare. Fami (B) averages about 5.2 on a 7-point scale, with a pretty low SD, suggesting that the words were mostly familiar. Unfortunately, here once again the computed numbers for kurtosis differ a lot. Just as explained above, this does not mean our reproduction is completely wrong. There are many factors that might influence kurtosis.

In summary, our reproduction shows the same patterns as the published table: most words are short, quite familiar, and positively skewed in their distributions. The small differences in skewness and especially in kurtosis are due to different calculation methods, not because our analysis is wrong. This is also a nice reminder that in statistics, sometimes numbers can vary slightly depending on the software or definitions used, but the overall interpretations remain the same.


# Descriptive statistics of the tests and reliability testing

In this chapter, we will attempt to reproduce the authors' descriptive
statistics regarding the two types of post tests and calculate
Cronbach's α to test reliability.

## Reliability analysis

```{r}
# Vocabulary A (L1 production)

# In this chunk, we conduct a reliability analysis for Vocabulary A scores of the L1 production dataset. For this to work, the psych package has to be installed and loaded:
# install.packages("psych", dependencies = TRUE)

library(psych) # -> reliability analysis/testing

# We apply the alpha() function for reliability testing.

alpha(VocabularyA_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary A (L2 production)

# The same function is used to conduct a reliability analysis for Vocabulary A of the L2 participants.

alpha(VocabularyA_L2production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L1 production)

# With Vocabulary B, we follow the same procedure.

alpha(VocabularyB_L1production[,-1], warnings=FALSE)

```

```{r}
# Vocabulary B (L2 production)

alpha(VocabularyB_L2production[,-1], warnings=FALSE)

```

These reliability analyses put out Cronbach’s α, a measure of internal
consistency of tests. It indicates whether responses are consistent
between items. Before interpreting these results, we will get to the
descriptive statistics of the two types of post-tests, which the
reliability analyses are in regard to.

## Accuracy

```{r}
# To investigate accuracy, we first introduce our dataset as the variable dat.acc and load the dplyr library. In accordance with the previous data imports, we are using the here package in this case as well: 

dat.acc<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)

library(dplyr) # -> for data manipulation

```

```{r}
## L1 production test
# L2 -> L1

# This code chunk, referring to L2 -> L1 of the L1 production set, processes the collected results for each participant — their ID, how many items they answered, and how many they got right — and organizes them into a clean table. Therefore, it gives us the core numbers necessary to describe and analyze accuracy. We want to take time to explain this chunk of code in detail. 


#L2→L1 (L1 production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

# The first line of code takes unique IDs from dat.acc and wraps them into a data frame called "ids". The column is then named "id" using the names() function. In the next step, that column is extracted as a vector, meaning that z is a vector of unique IDs. Further, a for loop is used to iterate over elements of this vector z and assigning the IDs to the variable "i". Inside this loop, for each i (a unique ID) the data is filtered for that participant under certain conditions and then the Answer column is selected. The result is stored as the object acc_recT, which is further converted into a vector a. With b and c, the processed answers are computed and lastly, these results are appended into three vectors Score, IDes, and try. 

accu_L2L1_L1Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L1Pro)<-c("ID", "Try", "Score")
accu_L2L1_L1Pro

# In the last part, the three vectors built before are combined into a final data frame, where each vector becomes a column in a table, each row corresponding to one participant. These columns are renamed and the table is printed. If we want to properly see the table outside of the console, we can use the View() function:

View(accu_L2L1_L1Pro)

```

```{r}
#L1→L2 (L1 production test)

ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L1 Production")%>%
    dplyr::select(Answer)-> acc_recT
  a<-as.vector(unlist(acc_recT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L1Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L1Pro)<-c("ID", "Try", "Score")
accu_L1L2_L1Pro
```

```{r}
# L1 production: Statistics and plot

# To provide descriptive statistics, we now want to use the describe() function on the Score column, which returns a rich set of stats.

#L2 → L1 learning
describe(accu_L2L1_L1Pro$Score)

#L1 → L2 learning
describe(accu_L1L2_L1Pro$Score)

# Plot (L1 production)

# To visualize this, we want to create a boxplot comparing the two learning directions. For this, the beeswarm package has to be installed and loaded:

# install.packages("beeswarm")

library(beeswarm)

# For this plot, we want to assign a data frame L1pro that contains the scores from both learning directions in the L1 production test. After changing the names of the columns to how we want them to appear on the x-axis, we create a side-by-side boxplot for the learning conditions. Finally, we add the beeswarm() function specifying add=T, which lets the individual scores appear as jittered dots that don't overlap.

L1pro<-data.frame(accu_L2L1_L1Pro$Score,accu_L1L2_L1Pro$Score)
names(L1pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L1pro,col="grey91", outline = T)
beeswarm(L1pro,add=T)
```

Boxplots are a way to visualize both the central tendency (median) of a variable and the spread around this central tendency (IQR). The median is represented by the thick line inside the box, while the box represents interquartile range, meaning the range of the middle 50% of the data. The whiskers outside the box extend to the highest and smallest values, and the jittered dots represent individual data points. As we can see here, the median is similar in both learning directions. L2→L1 displays slightly greater variability and a higher concentration of upper outliers. Overall, performance appears comparable between the two groups.

```{r}
## L2 production test
# L2 -> L1

# This chunk of code follows the same steps as before with the L1 production set, using L2 scores.

#L2→L1 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L2L1", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L2L1_L2Pro<-data.frame(IDes,try, Score)
names(accu_L2L1_L2Pro)<-c("ID", "Try", "Score")
accu_L2L1_L2Pro
```

```{r}
## L2 production test
# L1 -> L2

#L1→L2 (L2 Production test)
ids<-data.frame(unique(dat.acc$ID))
names(ids) <- ("id")
z<-ids$id
Score<-c()
IDes<-c()
try<-c()
for (i in z){
  dat.acc%>%
    dplyr::filter(ID == i, Condition=="L1L2", Test=="L2 Production")%>%
    dplyr::select(Answer)-> acc_proT
  a<-as.vector(unlist(acc_proT))
  b<-sum(a)
  c<-length(a)
  Score<-c(Score, b)
  IDes<-c(IDes, i)
  try<-c(try,c)
}

accu_L1L2_L2Pro<-data.frame(IDes,try, Score)
names(accu_L1L2_L2Pro)<-c("ID", "Try", "Score")
accu_L1L2_L2Pro
```

```{r}
# L2 production: Statistics and plot

# In this chunk of code, we want to provide descriptive statistics for L2 production test as well and visualize them in a similar boxplot.

# L2 -> L1 learning

describe(accu_L2L1_L2Pro$Score)

# L1 -> L2 learning

describe(accu_L1L2_L2Pro$Score)

# As with L1, we create a boxplot comparing the two learning directions in L2 production using the beeswarm package.

#Plot
L2pro<-data.frame(accu_L2L1_L2Pro$Score,accu_L1L2_L2Pro$Score)
names(L2pro)<-c("L2→L1 learning","L1→L2 learning")
boxplot(L2pro,col="grey91", outline = T)
beeswarm(L2pro,add=T)

```

Median scores in the L2 production test are similar for both learning directions (L2→L1 and L1→L2), suggesting comparable central performance. L1→L2 learning shows a slightly higher median. In the L2→L1 group, values below the median are more spread out, indicating more variety.
Comparing L1 and L2 production tests, L1 production scores appear to be generally higher and more consistent across learning directions.

```{r}
# Summarized results of descriptive statistics

# In this chunk, we want to create tables that display the summarized results of descriptive statistics, similar to the table used in the paper (p. 1126).

# First, we need to load the knitr and kableExtra packages.

library(knitr)
library(kableExtra)

# Next, we want to assign a variable that contains the scores for both data sets (L1 and L2 production) and use the describe() function to get all the important measures.

descriptive_stats_tests <- data.frame(L1pro, L2pro) |> 
  describe()

# In keeping with the table in the paper, the first two columns (number of observations n = 28, and vars) are removed. Also, the rows are renamed in a more readable way.

descriptive_stats_tests_trimmed <- descriptive_stats_tests |> 
  select(-n, -vars, -trimmed, -mad, -range, -se)

rownames(descriptive_stats_tests_trimmed) <- c("L2 → L1 (L1pro)", "L1 → L2 (L1pro)", "L2 → L1 (L2pro)", "L1 → L2 (L2pro)")

# Now we want to display the results in a clean, formatted table using the kable() function.

kable(descriptive_stats_tests_trimmed, 
      caption = "Descriptive Statistics for Test Scores",
      digits = 2) |> 
  pack_rows(index = c("L1 production test" = 2, "L2 production test" = 2)) |> 
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))


# Now we have a table that summarizes the descriptive statistics of the two types of post-tests, where the measures of scores in the two production tests can be easily compared. In the L1 production test, scores were shown to be generally higher than in L2 production test.

```

```{r}
# In this chunk, we want to summarize results of the reliability testing and reproduce a table similar to the one in the paper (p. 1126).

alpha_A_L1 <- alpha(VocabularyA_L1production[,-1], warnings=FALSE)
alpha_A_L2 <- alpha(VocabularyA_L2production[,-1], warnings=FALSE)
alpha_B_L1 <- alpha(VocabularyB_L1production[,-1], warnings=FALSE)
alpha_B_L2 <- alpha(VocabularyB_L2production[,-1], warnings=FALSE)



alpha_table <- data.frame(
  Vocabulary = c("Vocabulary A", "Vocabulary B"),
  
  Alpha_L1 = c(
    sprintf("%.2f", alpha_A_L1$total$raw_alpha),
    sprintf("%.2f", alpha_B_L1$total$raw_alpha)
  ),
  CI_L1 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L1$feldt$lower.ci, alpha_A_L1$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L1$feldt$lower.ci, alpha_B_L1$feldt$upper.ci)
  ),
  
  Alpha_L2 = c(
    sprintf("%.2f", alpha_A_L2$total$raw_alpha),
    sprintf("%.2f", alpha_B_L2$total$raw_alpha)
  ),
  CI_L2 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L2$feldt$lower.ci, alpha_A_L2$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L2$feldt$lower.ci, alpha_B_L2$feldt$upper.ci)
  ),
  
  stringsAsFactors = FALSE
)

# Rename column headers (use empty string for "Vocabulary")

colnames(alpha_table) <- c("", "Alpha", "95% CI", "Alpha", "95% CI")

# Now we want to display the results in a clean, formatted table.

alpha_table %>%
  kable(align = "lcccc", caption = "Alpha coefficients for L1 and L2 production test") %>%
  add_header_above(c(" " = 1, "L1 production" = 2, "L2 production" = 2))

# In this chunk, we calculated Cronbach’s alpha and 95% confidence intervals for two vocabulary tests (A and B) at two proficiency levels (L1 and L2), and created a summary table showing the reliability estimates and CIs for each test and level. Aside from the alpha() function, we used sprintf() to create and format strings for the table, which was then generated using the kable() function. 

```

If we compare this table about alpha coefficients with the one in the paper (p. 1126), we notice a difference in confidence intervals, specifically in the hundredths place. If we look at the output of alpha(), we see that it puts out two kinds of confidence intervals: Feldt and Duhachek. Reading up in the help file of the alpha() function, it becomes clear that alpha.ci (used to access CIs from alpha function) finds CIs using the Feldt procedure, which is based on the mean covariance, while Duhachek procedure considers the variance of the covariances. In the help file it is stated that in March, 2022, alpha.ci was finally fixed to follow the Feldt procedure. Since the paper was published in 2021, this might explain the deviating CIs here. If one would like to look at Duhachek's CI instead, it can either be seen in the output of alpha(), or one might install an earlier version of the package. Since we want to use the current psych package for our project and the differences don't change our outcome, we decided to stick with Feldt's CI and simply wanted to note why this difference occurs here.

As mentioned before, Cronbach's alpha indicates whether responses are consistent between items, and ranges between 0 and 1, a higher value meaning higher reliability.
As we can see in the results of the reliability analysis, alpha coefficients range from .73 to .84, showing adequate reliability of all the tests.


```

```{r}
# In this chunk, we want to summarize results of the reliability testing and reproduce a table similar to the one in the paper (p. 1126).

alpha_A_L1 <- alpha(VocabularyA_L1production[,-1], warnings=FALSE)
alpha_A_L2 <- alpha(VocabularyA_L2production[,-1], warnings=FALSE)
alpha_B_L1 <- alpha(VocabularyB_L1production[,-1], warnings=FALSE)
alpha_B_L2 <- alpha(VocabularyB_L2production[,-1], warnings=FALSE)



alpha_table <- data.frame(
  Vocabulary = c("Vocabulary A", "Vocabulary B"),
  
  Alpha_L1 = c(
    sprintf("%.2f", alpha_A_L1$total$raw_alpha),
    sprintf("%.2f", alpha_B_L1$total$raw_alpha)
  ),
  CI_L1 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L1$feldt$lower.ci, alpha_A_L1$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L1$feldt$lower.ci, alpha_B_L1$feldt$upper.ci)
  ),
  
  Alpha_L2 = c(
    sprintf("%.2f", alpha_A_L2$total$raw_alpha),
    sprintf("%.2f", alpha_B_L2$total$raw_alpha)
  ),
  CI_L2 = c(
    sprintf("[%.2f - %.2f]", alpha_A_L2$feldt$lower.ci, alpha_A_L2$feldt$upper.ci),
    sprintf("[%.2f - %.2f]", alpha_B_L2$feldt$lower.ci, alpha_B_L2$feldt$upper.ci)
  ),
  
  stringsAsFactors = FALSE
)

# Rename column headers (use empty string for "Vocabulary")

colnames(alpha_table) <- c("", "Alpha", "95% CI", "Alpha", "95% CI")

# Now we want to display the results in a clean, formatted table.

alpha_table %>%
  kable(align = "lcccc", caption = "Alpha coefficients for L1 and L2 production test") %>%
  add_header_above(c(" " = 1, "L1 production" = 2, "L2 production" = 2))

# Rundungsfehler?? why? vllt weil wir bereits in data frame gerundet haben? versuche dataframe mit 3f und dann erst in Tabelle runden

#whether responses are consistent between items
#*We want to associate the results of our analysis here with the interpretation in the study. As we can see in the results of the reliability analysis, which are also displayed in a clean table in the paper (see p.1126), alpha coefficients range from .73 to .84, showing adequate reliability of all the tests.
```


# Effects of Learning Condition (Research Question 1)

As we recall from the introduction, the first research question of Terai
et al.'s study is: Which learning direction is more effective for
vocabulary acquisition, L1 to L2 learning or L2 to L1 learning?
Therefore, the first model was built to analyze the relationship between
the production tests and learning conditions. It contained Learning
Condition and Test Type as explanatory variables, as well as the
interaction of the two variables. Random effects (Subject and Item) were
included, and production test answers were used as the response variable
(p. 1125).

```{r}
# Before starting on the model, we install and load the packages we need. These seem to be from an older Rstudio version, but they still work just fine!

# install.packages("lme4", discrepancies = TRUE)
# install.packages("effects", discrepancies = TRUE)
# install.packages("emmeans", discrepancies = TRUE)
# install.packages("phia", discrepancies = TRUE)
library(lme4) ##leading required package:Matrix/ this package is for mixed models
library(effects) # -> package is for plotting model effects
library(emmeans) # -> package is for post-hoc comparisons
library(phia) # same as emmeans

```

Now, for the first model, we want to go through the process step by step and connect the code with the interpretation of data in the paper.

```{r}


## Model 1: Test Type & Learning Direction

# To test RQ1, we fit a logistic mixed-effects regression model predicting accuracy (Answer) from Test, Condition, and their interaction. First, we want to fit a model without interaction.

# Model without interaction
fit1<-glmer(Answer~Test+Condition+(1|ID)+(1|ItemID),family=binomial, data=dat, glmerControl(optimizer="bobyqa"))

summary(fit1)

# With the glmer() function, we fit a generalized linear mixed-effects model. In the formula of this function, both fixed effects (Test answers and Condition) and random effects (ID and ItemID) are specified. (1 | ID) allows for a random intercept for each level of the variable ID (participants), (1 | ItemID) does the same for each level of the variable ItemID (test items). This accounts for repeated measures and inter-individual and inter-item variability. With specifying family=binomial, we tell the glmer() function that our response variable is binary (correct answer = 1, incorrect answer = 0). In the last argument, an optimizing algorithm is specified. We use the summarize() function to look at the output of this model.

# Setting number of significant digits and calculating AIC
options(digits=7)
AIC(fit1)
```

Without interaction, the model assumes that the effects of Test and Condition are additive and do not depend on each other. AIC (Akike Information Criterion) was calculated for both variants of the model. It is used to estimate the quality of a model in comparison to another one, where a lower AIC is better. Therefore, we will get back to it after fitting the model with interaction and compare the two.

If we look at the output of the summary() function for our model, the output includes *scaled residuals*. Residuals are the differences between observed and predicted values. This section shows summary statistics of the model's residuals after scaling: Minimum, first quartile (25^th^ percentile), median (50^th^ percentile), third quartile (75^th^ percentile), and maximum. These values give an impression of how well the model fits the data.
The "Random effects" section put out the estimated variability in the intercept across subjects (ID) and items (ItemID), accounting for differences in overall accuracy between participants and between items.



```{r}
# Model with interaction

# Now we fit a model similar to the first one, but with the difference that it includes the interaction between Test and Condition, investigating if the effect of one variable depends on the level of the other variable.

fit1.1<-glmer(Answer~Test*Condition+(1|ID)+(1|ItemID),family=binomial, data=dat, glmerControl(optimizer="bobyqa"))
summary(fit1.1)

# Computing confidence intervals

confint(fit1.1)

# Setting number of significant digits and calculating AIC

options(digits=7)
AIC(fit1.1)
```

Now after calculating AICs for both versions of the model, we can see that the model *with* interaction showed lower AIC (with interaction: 2427.102; without interaction: 2429.771), indicating higher quality of the model.

As mentioned above, participant and item intercepts were used as crossed random effects. To interpret the results in accordance with the paper, we want to look at the summary output of fit1.1 under Fixed effects. The results show a significant main effect of **Test Type** (Estimate = -0.976, SE = 0.105, z = -9.315, p < .001). These numbers can be found in the "Test2" row, and are all rounded up to three digits for better readability. The negative estimate tells us that accuracy in Test 2 is lower than in Test 1. The p-value can be extracted from the column marked Pr(>|z|), where it is displayed in the form of <2e-16, since it is such a small number. It tells us that the effect (of switching between test types) is significant. The z value tells us how many standard deviations away a value is from the mean.
Moving on to **Learning Condition**, there was no main effect, as we can see under "Condition2" (Estimate = -0.038, SE = 0.103, z = -0.366, p = .714). In this case, the p-value is > 0.05, meaning that the effect of learning condition is *not* significant. The main effect estimate of Condition2 is quite small (close to 0), telling us the predicted odds don't change much when moving from one learning condition to the other. 
The **interaction** of Test Type and Learning Condition (see row "Test2:Condition2") was also significant
(Estimate = -0.446, SE = 0.206, z = -2.169, p = .030). The interaction term tells us how the effect of Test on accuracy changes depends on the Condition, or the other way around. The estimate tells us that, when both Test2 and Condition2 are present, the accuracy is even lower than when only one of them is. The p-value of .030 indicates that the interaction effect is unlikely due to chance. 

```{r}
# Testing interaction

How does L2 proficiency influence the effects of L2->L1 and L1->L2 learning?

The authors hypothesized that the effect of learning direction might be dependant on the learner's proficiency in English. Meaning, lower-proficiency learners might benefit more from practicing in the L2->L1 direction, whilst higher-proficiency learners might benefit more from the L1->L2 direction.

To answer and test this question, we add an interaction term between Condition (learning direction) and s.vocab (scaled proficiency). 

We use a generalzed linear mixed model (GLMM) with a logistic link function, because the dependent variable (Answer) is binary (correct = 1, incorrect = 0).(And the authors fit generalized linear mixed models (GLMMs) separately for the L1 and L2 production test.)

GLMMs are appropriate and quite useful here because:
- The dependent variable (Answer) is binary (correct=1, incorrect=0).
- Responses are nested: the same participants answer multiple items, and the same items are seen by multiple participants.

The packages that we need here are:
- dplyr for the function filter() and general data manipulation,
- lme4 for the function glmer() for fitting GLMMs,
- effects for the function allEffects(), for plotting model predictions,
- phia, for the function testInteractions(), for probing interactions.

The paper reports that the advantage flips by proficiency level
(lower-proficiency learners tend to benefit more from L2-\>L1, higher
proficiency from L1-\>L2).


# In this chunk, we want to move on to further analyze the two models and their results. We will start with comparing the model with and without interaction using the anova() function:

anova(fit1,fit1.1)

# Checking simple main effects in both directions
testInteractions(fit1.1, fixed="Test", across="Condition")

testInteractions(fit1.1, fixed="Condition", across="Test")
```

Since a significant effect of the interaction was found, we moved on to comparing the two models with anova() to test if the interaction improves the model. It puts out a p-value of 0.03071. Being smaller than 0.05, this result is statistically significant, which is also indicated by the * specified in the significance codes in the output. It tells us that fit1.1 (model with interaction)  fits the data significantly better than fit1 (without interaction). 
With the testInteractions() function it is possible to perform simple main effects analysis after finding a significant interaction. A significant interaction only tells you that the effect of one variable depends on the level of the other, not which levels are different. So with this function, we want to unpack the interaction further and find out how the effect of one variable (Test or Condition) differs across levels of the other. If we look at the output of the first function where Test is used as the fixed variable, the p-values (L1 Production: 0.188; L2 Production: 0.164) tell us that the effect of Test does not significantly differ between conditions. If we take Condition as the fixed variable, p-values are very small (L1L2: 1.931e-07; L2L1: 1.608e-15), indicating that the effect of Condition does significantly differ between tests.

```{r}

# Multiple comparisons
# Through computing estimated marginal means using the emmeans() function, we want to conduct pairwise comparisons of Condition levels within each level of Test, and assign it to a variable a.
a<-emmeans(fit1.1, pairwise~Condition|Test, adjust="tukey")
  a
  
# Computing confidence intervals for emmeans object a
confint(a,parm,level=0.95)

# Calculating effect sizes
eff_size(a,sigma=sigma(fit1.1), edf= Inf)

# The same procedure is now applied in the other direction: While with variable a, we told emmeans() to do pairwise comparisons of Condition levels within each level of Test, we now want all comparisons of Test levels within each level of Condition, and save this object as the variable b.

# Computing EMMs 
b<-emmeans(fit1.1, pairwise~Test|Condition, adjust="tukey")
  b
  
# Computing confidence intervals for emmeans object b
confint(b,parm,level=0.95)

# Calculating effect sizes
eff_size(b,sigma=sigma(fit1.1), edf= Inf)

```

The analysis conducted in the code chunk above provides the most important information for the interpretation of this model done by the authors. We want to go through their statements and connect them with the code we reproduced. 
In the paper, the authors claim there was a statistically significant difference between the scores of the two tests, suggesting that L1 production test scores were higher than L2 production test scores in both learning conditions (**L2 to L1 learning:** p < .001, d = 1.20, 95% CI [0.91, 1.49]; **L1 to L2 learning:** p < .001, d = 0.75, [0.47, 1.04]) (p. 1126). 
Now, we want to take a close look at these numbers and what they mean. In our code above, we can find the numbers stated here in the output regarding our emmeans object b. The p-values show up when printing b, and lie < .001 for both learning conditions. A small p-value tells us that an effect likely exists, but does not say much about how big or important it is. This is where effect size comes into play: The function eff_size() puts out the estimated effect sizes for both conditions, which are referred to as **d** in the paper. They both demonstrate a large (larger for L2 to L1) effect size, and indicate that the difference (different levels of Test within each Condition) is not just statistically significant, but meaningful in size. Lastly, the confint() function applied to b put out the 95% confidence intervals for both learning directions, which means we can be 95% confident that the true standardized effect size lies in this range (which is a meaningful effect size, in our case).

It is further expressed that results showed no main effects of Learning Condition (L1 production test: p = .188, d = -0.19, 95% CI [-0.46, 0.09]; L2 production test: p = .082, d = 0.26, [-0.03, 0.55]). This can be derived when we look at object a, which compares learning conditions separately for L1 Production and L2 Production. Printing a puts out p-values, which lie above 0.05 in both production tests and indicate no statistical significance of the differences between learning conditions. The effect sizes show a small negative effect for L1 Production, and a small positive effect for L2 Production, and do not suggest a meaningful difference. The confidence intervals in both cases include 0 (since it lies between -0.03 and 0.55, for example), telling us that no effect (d = 0) is a possibility given the data. To sum up, all these values do not support a main effect of Learning Condition.

```{r}
# Visualizing the first model
# Finally, we want to present the results of this model in a plot that visualizes all important information regarding our research question.

# Setting graphical parameters for the plot
par(pty="s") # sets the shape: square
par(pin=c(10,10)) # sets the plot size
par("ps") # font size

# Plot
plot(allEffects(fit1.1),multiline=T,ci.style="bands", xlab="Test Type", ylab="Accuracy Rate",
       main= "Test & Learning Direction", lty=c(1,2), rescale.axis=F, ylim = c(0,1),
       colors="black",asp=1)
```

Lastly, the model is visualized. The plot shows the interaction between Test Type and Learning Condition on accuracy rates, based on the generalized linear mixed-effects model (fit1.1). It is an effect plot generated with the allEffects() function. The solid line represents the L1 to L2 learning condition; the dotted line represents the L2 to L1 learning condition. The y-axis represents accuracy rates on both L1 and L2 production tests. The x-axis represents Test Type.
We can see that the accuracy rates differ across Test Types and Learning Conditions, with a clear interaction between the two: The non-parallel lines indicate that the effect of one variable changes depending on the level of the other variable. The distance between the two lines is relatively small at each test type, which reflects the results that revealed no significant difference in learning effects between L2 to L1 learning and L1 to L2 learning.


```{r}
# Extension (Hannah)
# To extend the work on this model, I want to display the results using the ggplot() function instead. For this, we need to load lme4, gglpot2 and paletteer libraries:

library(lme4)
library(ggplot2)
library(paletteer)

# I want to present the values of b$emmeans, which contains estimated marginal means. ggplot works with data frames, not with glm objects. If we check the structure of our object, it returns an emmGrid object:

str(b$emmeans)

# To create a ggplot, we need to transform the EMMs back to the original scale of the response variable with specifiying the type, and then convert our emmeans object into a data frame:

b <- emmeans(fit1.1, pairwise ~ Test | Condition, adjust = "tukey", type = "response")

emm_df <- as.data.frame(b$emmeans)

# Plot using ggplot2
ggplot(emm_df, aes(x = Test, y = prob, group = Condition, color = Condition, linetype = Condition)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL), width = 0.1) +
  scale_color_paletteer_d("beyonce::X11") +
  scale_linetype_manual(values = c("solid", "dashed")) +
  scale_y_continuous(name = "Accuracy Rate", limits = c(0.0, 0.75)) +
  xlab("Test Type") +
  ggtitle("Test & Learning Direction") +
  theme_minimal() +
  theme(
    text = element_text(size = 14),
    plot.title = element_text(hjust = 0.5)
  )
# This plot is quite similar to the one presented in the author's code. The only addition in *what* is presented, is that it contains so-called error bars above and below each point, which reflect the confidence intervals. Most differences lie in *how* the data is presented, where I changed the appearance a bit. I added the Beyonce palette, to give the two lines (that represent Learning Condition) some color. I still kept them solid vs. dashed, so that they can be told apart easily. Also, the legend for "Condition" is next to the plot, not above it, which I find more well-arranged.
```

Even though this is an additional plot to the one the authors already provided in their code, I want to explain why using ggplot2 for this might be of advantage. ggplot2 is built on the "grammar of graphics", which makes it easy to add layers and extend the plot. There are many options to alter the appearance of the plot, like colors, themes etc. Furthermore, in this code, we can explicitly see what is plotted and how specific elements are modified. This makes the plot more reproducible and easier to update or change later on.






# Effects of Learning Directions and L2 Proficiency (RQ2)

As introduced above, the second research question asks: How does learners' L2 proficiency change the relative benefits of L2 -> L1 vs. L1 -> L2 learning?

Terai et al. (2021) hypothesized that the effect of learning direction might be dependant on the learner's proficiency in English. Meaning, lower-proficiency learners might benefit more from practicing in the L2 -> L1 direction, whilst higher-proficiency learners might benefit more from the L1 -> L2 direction.

To answer and test this question, the authors once again use generalized linear mixed models (GLMMs) separately for the L1 and L2 production tests, with Condition (learning direction), scaled vocabulary size (s.vocab), and their interaction as fixed effects, and Subject/Item random intercepts.

Below, we will go through the code step by step to try and understand what the authors did to answer this research question.


##L1 Production test

(((With a second model the scores of the L1 production test were analyzed. This second model contained Learning Condition and Vocabulary Size (English proficiency) as explanatory variables and Accuracy of the L1 production test as the response variable (Terai et al. 2021 : 1127). Here the authors analyzed the data with interaction and without interaction, to find out the AIC, p and z scores there are a few steps we need to take. )))

Step 1: Model setup and Data Preparation

We begin by focusing only on the L1 production data. This can be done by first loading our data with the command read.csv() and then the command filter() which only keeps the rows for the L1 Production test.


```{r}

#First, we filter the data so that only the relevant test type (L1 or L2 Production) remains. Then, we ensure that categorical predictors are properly set as factors and that numeric predictors are scaled (standardized).
dat1 <- read.csv(file = here("data", "dataset1_ssla_20210313.csv"), header=T)


##With the filter(...) function, provided by the dplyr package, we only keep the rows for the L1 Production posttests, because we will analyze this test separately from the L2 production test. Meaning, the function is mainly used to subset rows in a data frame based on our specified conditions, (L1 production Test)
dat1.L1<-filter(dat1, Test=="L1 Production")


#Make the categorical factors factorial: this means qualitative data is encoded into numeric data so that we can work with it and use the data for statistical analysis. The as.factor() function is used to convert a vector into a factor, which is a data type specifically designed for categorical data. It is essential to use factors for statistical data analysis such as an ANOVA or a regression which we will be doing in a later step. At this point, we need factors for a summary of our data.
#Very basic: We set Condition (Learning direction) as a factor; Condition represents the learning direction (L1 -> L2 or L2 -> L1)
dat1.L1$Condition <- as.factor(dat1.L1$Condition)
```


We also apply contrast coding. Centered contrasts, in our case -0.5, +0.5, make the intercept much easier to interpret, since it then represents the grand mean across conditions. 


```{r}
  
#In this next step we change our coding so the variables are correct and not still working with the data from previous tests. 
#The function contr.treatment() is used to create contrast matrices for categorical variables in regression models. It is the default contrast coding method in R for unordered factors. This way a baseline/reference level is assigned and then it compares all the other levels to this baseline/reference.
c <- contr.treatment(2)


#The matrix() function creates matrices, which are two-dimensioal data structures where all elements must be of the same type (numeric, character, logical...). ncol=1 is the Number of columns.
my.coding <- matrix(rep(1/2,2,ncol=1))

my.simple <- c-my.coding

#The print() function can be used to display the value of an object or expression in the console. It can be customized for different object classes. In our case, it prints out what we have assigned to my.simple.
print(my.simple)
##  2
## 0.5
##-0.5

##makes main effects easier to interpret and reduces collinearity in interaction models (so wie ich es verstanden habe????)
#The function contrasts() in R specifies or even retrieves the contrast coding for a variable.
#Contrast coding is essential when working with linear models, as this can determine how how levels of a factor are represented as numeric dummy variables.
#A contrast in R is a combination of variables that allows comparison of different treatments.. (Explain more? Understand??)
#By applying contrast coding, the two groups (L1 to L2 and L2 to L1) are coded as -0.5 and +0.5. This coding makes the model's intercept easier to interpret (it becomes the overall mean).
contrasts(dat1.L1$Condition)<-my.simple

```


In the next step, we need to standardize the vocabulary size scores. That way, one unit of s.vocab corresponds to one standard deviation of proficiency. (REWRITE)


```{r}
#Scaling the Score: Here Vocab scores (L2 proficiency measures) are converted to z-scores. The scale() function standardizes or normalizes data. It centers or scales the columns of our numeric matric/data frame.
dat1.L1$s.vocab<-scale(dat1.L1$Vocab)

```

##Step 2: Model without interaction

First, we test a model which includes learning direction and proficiency as main effects only.

This first model assumes that learning direction and L2 proficiency each have independent effects on test accuracy.


```{r}
#Answer~Condition*s.vocab: includes main effects of learning condition and proficiency plus their interaction
#(1|ID) + (1|ItemID): random intercepts for participants and items (to account for repeated measures)
#family = binominal: logistic regression, appropriate for binary outcomes

fit2<-glmer(Answer~s.vocab+Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))

summary(fit2)

```

```{r}
options(digits=7)
AIC(fit2) 
##This command prints AIC, a fit index (the lower the better)
```


##Interpretation
The model predicts the probability of a correct response (Answer), using logistic regression. The random intercepts (1|ID) and (1|ItemID) account for individual differences between
participants and between vocabulary items. The output of this first model shows that s.vocab is not significant with p = .57, this means that proficiency by itself did not strongly predict L1 Production accuracy. For 'Condition' the model also showed that it was not significant p = .18, meaning that learning direction alone also did not explain differences. So if we look at each predictor separately, there seems to be no clear effect.


##Step 3: Model with Interaction

What if proficiency only matters depending on the learning direction? To figure this out, we add an interaction term. 


```{r}
fit2.1<-glmer(Answer~s.vocab*Condition+(1|ID)+(1|ItemID), family=binomial, data=dat1.L1, glmerControl(optimizer="bobyqa"))
summary(fit2.1)
```

```{r}
confint(fit2.1)
```

```{r}
options(digits=7)
AIC(fit2.1)
##addition of s.vocab:Condition which directly tests RQ2: Does the effect of learning direction depend on L2 proficiency? -> here s.vocab:Condition2 is negative and significant (-0.367, p=.009)
```

The interaction term s.vocab:Condition is significant with an estimate of about -0.37, p = .009, which means that the effect of vocabulary proficiency depends on the learning direction. With centered coding, a negative interaction such as -0.37 suggests that the slope of proficiency is steeper for L1 -> L2 than for L2 -> L1. Very basically said: at a lower proficiency, L2 -> L1 shows a higher accuracy, while at a higher proficiency, L1 -> L2 improves. The crossover in the paper occurs around vocabulary size of about 5.419 which confirms the hypothesized 'flip'. Effecctiveness of learning direction depends on proficiency.


##Step 4: Model comparison

To formally check which model fits better using AIC and a likelihood ratio, we use to command anova(), which is an analysis of variance (a statistical formula which compares variances across the means (or average) of different groups).

```{r}
anova(fit2,fit2.1)
```


The AIC favors the interaction model (1311.6 vs. 1316.3), and the likelihood ratio is statistically significant (χ² = 6.70, p = .009), indicating that the model with the Condition x Proficiency interaction fits best.



##Step 5: Visualizing the effect
```{r}
plot(allEffects(fit2.1), multiline=T, ci.style="bands",
    xlab="Vocabulary Size", ylab="Accuracy Rate",
    main="L1 Production", lty=c(1,2),
    rescale.axis=F, ylim = c(0,1), colors="black", asp=1)
```

Looking at the plot you see a nearly flat line for L2 -> L1 and a positive, steeper line for L1 -> L2 which cross at intermediate proficiency. This plot shows that accuracy increases with proficiency. 

The slope differs by condition. It is much steeper for L1 -> L2. For lower proficiency learners, L2 -> L1 is better, and for higher proficiency learners L1 -> L2 is more beneficial. Learners in one learning direction seem to benefit more strongly from larger vocabularies.


##Step 6: Testing simple effects

This tests whether the slope of proficiency differs significantly between conditions as predicted. This is basically a Chi-square test.

```{r}
testInteractions(fit2.1, fixed="s.vocab", across="Condition")
```

##Interpretation: ??



##L2 Production Test

We now repeat all the same steps for the L2 production test.

```{r}
dat2<-read.csv(file = here("data", "dataset1_ssla_20210313.csv"))

dat2.L2<-filter(dat2, Test=="L2 Production")


dat2.L2$Condition<-as.factor(dat2.L2$Condition)

  
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2,2,ncol=1))
my.simple<-c-my.coding
print(my.simple)
##  2
## 0.5
##-0.5
```

```{r}
contrasts(dat2.L2$Condition)<-my.simple 

dat2.L2$s.vocab<-scale(dat2.L2$Vocab)


fit3<-glmer(Answer~s.vocab+Condition+(1|ID)+(1|ItemID),family=binomial, data=dat2.L2, glmerControl(optimizer="bobyqa"))
summary(fit3)
```

```{r}
options(digits=7)
AIC(fit3)
```

```{r}
fit3.1<-glmer(Answer~s.vocab*Condition+(1|ID)+(1|ItemID),family=binomial, data=dat2.L2, glmerControl(optimizer="bobyqa"))
summary(fit3.1)
##addition of s.vocab:Condition which directly tests RQ2: Does the effect of learning direction depend on L2 proficiency? -> here s.vocab:Condition2 is negative and significant (-0.367, p=.009)
            
```


#Here the results showed that the interaction s.vocab:Condition is significant (-0.367, p = 0.009). This means that the effects of proficiency depend on the learning direction. Vocabulary knowledge seems to help more in one condition than in the other.

```{r}
confint(fit3.1)
```

```{r}
options(digits=7)
AIC(fit3.1)
```

#Model Comparison
#To formally check whether the model with the interaction is better than the simpler model, we compare them with a likelihood ratio test:

```{r}
anova(fit3,fit3.1)
```


##Plotting/Visualizing the Effect using the effects package

```{r}
plot(allEffects(fit3.1),multiline=T,ci.style="bands", xlab="Vocabulary Size", ylab="Accuracy Rate",
       main= "L2 Production", lty=c(1,2), rescale.axis=F, ylim = c(0,1),colors="black",asp=1)
```

The same trend seems to appear for L2 production (flatter L2 -> L1 slope, steeper L1 -> L2 slope), but the interaction in this case is not significant (p = 0.70). The authors note that this might be due to a floor effect which means that an L2 form recall may be more difficult after a brief ecposure, so the overall performance is lower and the interaction is more difficult to detect statistically. 

##Interpretation & Take-home

Generally, L1 production showed that lower proficiency learners benefitted from L2 -> L1 learning direction, while higher proficiency speakers benefitted from L1 -> L2 learning direction (crossover about 5.419). For the L2 production similar things were obsered but were not statistically significant.

Vocabulary proficiency shapes how effectively retrieval practice works, and the optimal learning direction may differ for learners with higher vs. lower proficiency.

The optimal learning direction is clearly not a one-size-fits-all matter, it depends on proficiency. 


What you learned in this section:
- Adding a continous predictor (s.vocab) to a GLMM.
- Testing and interpreting interactions.
- Comparing models using AIC and likelihood ratio tests.
- Visualizing interaction effects with the effects package.


#Model Diagnostics

Now, we checked our GLMMs and have our results. But before interpreting these results too confidently, we should also check whether the predictors are highly correlated with one another which is a problem called multicollinearity. Multicollinearity might affect the results of our GLMMs.

Why does this matter?

- If our predictors are strongly correlated, the model has trouble keeping apart their unique contributions. 
- This inflates standard errors, making estimates unreliable. 

A common diagnostic here is the Variance Inflation Factor (VIF). A VIF close to 1 means predictors are not collinear. Rules:
-> VIF = 1: perfect independence
-> VIF > 5: potential concern
-> VIF > 10: serious multicollinearity

#Calculating VIFs for the Models

We use a helper function vif.mer() to calculate VIFs for mixed models.

```{r}
vif.mer(fit1.1)

vif.mer(fit2.1)

vif.mer(fit3.1)
```

Output:

fit1.1 (Test x Condition):

fit2.1 (L1 Production):

fit3.1 (L2 Production):

#Interpreting the VIFs

All our VIF values are obviously quite close to 1. This means that the predictors (s.vocab, Condition, and their interaction) are not correlated with each other in problematic ways. Therefore our models are statistically stable: the estimate of the interaction effects we observed in our GLMMS are not due to multicollinearity.

We can confidently say that the interaction between learning direction and proficiency is a genuine effect, not an artifact of overlapping predictors. The models are behaving well in terms of collinearity. 




# Conclusion

You are now familiar with handling generalized linear mixed models! You have learned how to fit them, interpret their main effects and interations, as well as checking their assumptions with VIFs. GLMMs are powerful tools for analyzing binary outcomes such as accuracy (SOURCE??) which allows us to account for repeated measures across participants and items. It is completely normal that at times GLMMs can feel intimidating: coding contrasts and interpreting the outcomes are not always straightforward. But similar to plotting, there is always a solution. The insights we can gain from a nice model make all the effort worthwhile. 

This chapter's analysis revealed that the effect of the learning direction is not fixed, but rather depends on L2 proficiency. In the L1 Production Test, learners with a lower-proficiency benefitted more from L1 to L2 practice, while learners with a higher-proficiency showed the opposite pattern. In the L2 Production Test we observed a similar tred, though the interaction was weaker. These results suggest that the learning direction may shift as learners' vocabulary knowledge grows. 

In this chapter we combined descriptive statistics, mixed-effects modeling, and visualizations to answer the study's research question. The answer is nuanced: both learning directions can be effective, but their relative benefits change across proficiency level. While working on this we also figured out how model comparison (AIC) and diagnostics (VIFs) helped ensure that our conclusions are statistically sound (?).

It seems that vocabulary learning is not something that can be generalized. While retrieval practice is robustly effective, the direction of practice depends on who the learner is. Due to the extensive research by Terai, Yamashita and Pasich (2021) who published their dataset and code along with the study, we were able to reproduce their findings and gain hands-on experience with modeling. This not only helps with our understanding of the research, but it also sharpens our skills as analysits of second-language data. 

---- 


-   No overall difference in accuracy between L1 -\> L2 and L2 -\> L1
    learning (RQ1)
-   A significant interaction between learning direction and L2
    proficiency for L1 production scores (RQ2)
    -   lower-proficiency learners benefited more from L2-\>L1 learning
    -   higher-proficiency learners benefited more from L1-\>L2 learning

Statistical takeaway (maybe???): Mixed-effects logistic regression
allows us to model data with both fixed effects (e.g. condition) and
random effects (e.g. participant and item variability)
